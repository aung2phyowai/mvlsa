\section{Motivation}
\label{sec:motivation}

If our goal is to find vector space representations or
\emph{embeddings} of words from multiple sources of data, then let us
enumerate the interpretations of GCCA to motivate why applying this
framework to our problem makes sense.

\begin{enumerate}[leftmargin=*]
\item \cite{horst1961generalized} provided the following
  interpretation. Assume that you have samples of $m$
  ``co-variates''; Then GCCA finds unit variance linear
  projections $Z$ such that some measure of the inter-projection ``correlation
  matrix'' $\Phi$ is maximized. Specifically we choose the measure to be
  the ``spectral-norm'' or the largest singular value of $\Phi$. This
  is referred to as ``MAX-VAR'' GCCA. A more illuminating
  interpretation of maximizing the ``spectral-norm'' of $\Phi$ is that
  by doing so we are finding $Z$ that can be best explained by a rank
  one approximation, in other words we are finding $Z$ that are most
  amenable to rank-one PCA, or that can be best explained by a single
  term factor model.
\item \cite{carroll1968generalization} derived ``MAX-VAR'' GCCA from a
  the following objective. They found an orthogonal
  representation $G$ of the co-variates $Z_j$ that was maximally
  correlated to them. Mathematically they maximized
   the expression $\sum_j \textrm{correlation}(G, Z_j)^2$. In words
   this expression is telling us to find the best middle ground
   representation $G$ between the projections $Z_j$ as measured by the
   norm of the correlation vector $\phi$. This seems like a sensible
   objective for the problem of learning lexical representation from
   multiple arbitary views/sources of data and therefore
   we use it.
\item \cite{bach2005probabilistic} presented a probabilistic
  intepretation for CCA. Though they did not generalize it to
  include GCCA we believe that one could give a probabilistic
  interpretation of ``MAX-VAR'' GCCA easily and we are working on
  it. We mention it, since a probabilistic
  interpretation would allow us to build a generative model and learn
  lexical representations unlike methods like Glove or LSA that rely
  solely on global term cooccurrence matrices and cannot calculate
  perplexity or generate sequences.
\item \cite{via2007learning} presented a neural network model of GCCA and
  adaptive/incremental GCCA. That approach is out of the scope of this work.
\end{enumerate}

Let us make a connection to multi-view learning to further motivate
why using a large number of views can help during learning. The aim of
multi-view learning is to leverage multiple views during training to
improve generalization. GCCA is a general multi view learning method that has the appealing
property that the representations it learns are invariant to affine
transformations of the input, which reduces the number of
pre-processing options. For instance, we don't have to choose 
between log(tf) and log(tf-idf) transformation since log(tf) is the
same as log(tf-idf) up to an affine transformation. Another benefit of
using GCCA over vanilla LSA is that since we can naturally 
fuse the statistics generated by using different window sizes we
don't have to specify an arbitray weighting method, like reciprocal
weighting, for creating a single cooccurrence matrix to represent a
corpus.


%% For example, we can define the rows to be English language words
%% and the columns to be the foreign language words and
%% count the number of times an English word aligns with a
%% foreign language word in a word aligned bitext corpus. Similarly,
%% a monolingual corpus provides multiple sources of 
%% cooccurrence statistics depending on the definition of the
%% context. By changing context of a word from the last word to 15th
%% previous word we can
%% create 15 different sources of non redundant cooccurrence
%% statistics. Even structured databases like WordNet and
%% FrameNet can be used to gather cooccurrence statistics and we would
%% give an example of how FrameNet can be used for this purpose later in
%% the paper. Finally, we can fuse representation generated using
%% multiple different algorithms with the goal that the fused representations should perform better
%% than the components.

In Section~\ref{sec:gcca} we present GCCA and a fast method to
compute it. 
Then we describe our Train and Test data in
Section~\ref{sec:data} followed by Experiments and Results in
Section~\ref{sec:exp} and conclude with Discussion and Future~Work. 
We call this method \textit{Multiview LSA} because we are using GCCA
which uses multiple views and because GCCA is a generalization of PCA and LSA
relies on PCA. 
