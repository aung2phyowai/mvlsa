\documentclass[11pt]{article}
\usepackage[ruled,]{algorithm2e}
\usepackage{acl2014}
\usepackage{times}
\usepackage{mathtools}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Regularized Generalized CCA to create
  representations from multiple sources of data}

\author{Pushpendre Rastogi \\
  Johns Hopkins University \\
  {\tt pushpendre@jhu.edu} 
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Real valued vector space representations of words have become an
  important research topic recently. Many researchers have reported
  that vector representations of words which place similar words close
  to each other are useful as features for tasks like pos tagging, NER and 
  parsing. In this paper we employ the framework of regularized generalized CCA
  to fuse new sources of co-occurence statistics and other representations to learn vector
   representations of words which outperform the base
   representations. The main aim of the paper is to present
   regularized GCCA as a natural and simple way that can fuse lots of
   data sources.
\end{abstract}

\section{Introduction}
Vector space representations of words which assign
 real vectors to words have been shown to enhance performance for nlp
 tasks recently. These representations learnt under diverse frameworks like
 SVD and Neural Networks make use of the co-occurence information
 present in text and are commonly evaluated on tasks
 like word similarity prediction, word analogy prediction and
 NER~\cite{dhillon2011multi,dhillon2012two,mikolov2013efficient,mikolov2013distributed,collobert2013word,zou2013bilingual,faruqui2014improving,pennington2014glove,bansal2014tailoring,levy2014dependency,levy2014linguistic,dhillon2011multi,dhillon2012two}.

In this paper we pose the problem of learning representations as the
problem of estimating latent factors/sufficient statistics that can explain the
co-occurrence statistics that may arise from a number of different
sources. For example cooccurence statistics can be extracted from
word aligned parallel bitext corpora by defining the columns to be the
foreign language words and the rows to be english language words and
counting the number of times an english word was aligned to a
particular foreign language word. Viewed this way every language pair between english
and a foreign language becomes a source of cooccurence
statistics. Similarly, even a single monolingual corpus made by
concatening all the text in this world can provide multiple sources of
cooccurence statistics depending on the definition of the
context. By changing the window size we can create an infinite number
of different sources of cooccurence statistics. Another source of
cooccurence statistics are structured databases like Roget Thesaurus,
WordNet and FrameNet which can be used to create cooccurence
statistics. Finally, we may want to fuse predefined vector
representations released by other
researchers in the hope that the resulting embedding would be better
than the components.\footnote{\url{http://lebret.ch/embeddings/200/words.txt}}
  \footnote{\url{http://nlp.stanford.edu/projects/glove/}}
  \footnote{\url{https://code.google.com/p/word2vec/}}
  \footnote{\url{http://metaoptimize.com/projects/wordreprs/}}
  \footnote{\url{http://ttic.uchicago.edu/~mbansal/data/syntacticEmbeddings.zip}}
  \footnote{\url{http://www.cs.cmu.edu/~mfaruqui/soft.html}}
  \footnote{\url{http://www.cis.upenn.edu/~ungar/eigenwords/}}


Regularized Generalized CCA is a well-studied statistical method which
can be used to find representations. Those representations have
guarantees of optimality under assumptions of normality however
empirically they work well under other distributions as well. The
focus of this work is to use RGCCA to fuse multiple large sparse
matrices of cooccurence statistics with existing representations and to
check the performance of the resulting representations.
An important reason why GCCA is appealing for this fusion is that it
learns affine invariant transformations which reduces the number of
pre-processing options. For example, we don't have to choose 
between log(tf) and log(tf-idf) transformation since log(tf) is the
same as log(tf-idf) upto an affine transformation. Also since we can
fuse the statistics generated by 
using different window sizes we don't have to tune for the best window
size. Also, we can easily fuse embeddings created using
different frameworks like word2vec, HPCA, ivLBL, Glove etc. into a single representation.

In Section~\ref{sec:gcca} we present GCCA and our notation. In
Section\ref{sec:rgcca} we present regularized GCCA 
 and an efficient way to compute it. Then we present data and experiments and results.

\section{Background}
\label{sec:gcca}
%% \subsection{CCA}
%% \label{ssec:cca}
%% In CCA the objective is to find optimal orthogonal subspaces of a given order such that once we
%% project two datasets X and Y to those subspaces then the correlation
%% between the projections is maximized amongst corresponding directions
%% and orthogonal to other directions.

%% For example, let's say we want to find subspaces of order 1
%% (i.e. single directions) a and b such that $a^\topX$ and $b^\topY$ are
%% maximally correlated. Here X and Y are vector valued random variables
%% and correlation between two random variables X and Y with zero mean is defined as,
%% $$O = \frac{E[X^\topY]}{\sqrt{E[X^\topX] \times E[Y^\topY]}}$$ By substituting this
%% formula to maximize correlation between $a^\topX$ and $b^\topY$ we get the objective as
%% $$\arg_{a,b}\max \frac{E[a^\topX \times b^\topY]}{\sqrt{E[(a^\topX)^2] \times E[(a^\topY)^2]}}$$
%% This problem of finding  a and b which maximize this objective is a
%% sub-problem of the generalized eigen value problem. Refer to
%% \cite{borga2001canonical} for more details.

%% We also note here that minimizing the euclidean distance between two
%% vectors when their magnitude are fixed is the same as maximizing their
%% correlation. For example, consider two vector valued random variables X and Y. The
%% expected euclidean distance between X and Y is
%% $$D = E[(X-Y)^\top(X-Y)] =  E[X^\topX + Y^\topY - 2 * X^\topY] $$
%% This is an informal way to give some intuition of how CCA is related
%% to the problem of Least Squares Regression. See \cite{hastie2009elements} for a detailed
%% explanation of the relation between CCA and problem of Least Squares.

%% We also make another note that CCA has a probabilistic interpretation
%% as finding the bayes estimates of a hidden factor given samples from
%% two dependent random variables. This interpretation was first
%% explained in \cite{bach2005probabilistic}.

%% \subsection{GCCA}
%% \label{ssec:gcca}
Generalized CCA
\cite{kettenring1971canonical,carroll1968generalization} is an
extension of CCA to the case when we have data from more than two
vector valued random variables. 

Let $X_j \in \mathbb{R}^{N\times d_j}$ be the mean centered matrices containing cooccurence statistics from view
$j$ where the rows represent words in the vocabulary and columns
represent the contexts. Therefore $N$ remains the
same across views but  $d_j$ varies.
We use the method of \cite{carroll1968generalization} to find the
 group configuration matrix $G \in \mathbb{R}^{N\times r}$, which is
 also our vector representation, and linear transformation matrices
 $U_j$ to solve 
\begin{equation}
  \label{eq:gcca}
\begin{split}
  \operatorname*{arg\,min}_{G,U_j} & \sum_{j=1}^J \begin{Vmatrix} G - X_jU_j^\top \end{Vmatrix}^2_F \\
  \text{such that } & G^\top G = I
\end{split}
\end{equation}
The solution for Equation~\ref{eq:gcca} is given by the following \cite{carroll1968generalization}.

\begin{eqnarray}
P_j &=& X_j(X_j^\top X_j)^{-1}X_j^\top \\
M &=& \sum_{j=1}^J P_j\\
M G &=& G \Lambda\\
U_j &=& \left(X_j^\top X_j\right)^{-1} X_j^\top G
\end{eqnarray}
We can see that $P_j$ is a projection matrix. Since eigenvalues of
projection matrices are either 0 or 1 that implies 
that eigenvalues of sum of M must lie between 0 and J.
Also a projection matrices is dense even when the original data matrix
is sparse. This leads us to the problem of finding the SVD of large
dense matrices which do not have a large absolute difference between
the highest and lowest eigenvalues.

\section{Computing Regularized GCCA}
\label{sec:rgcca}
Following the notation of \cite{hastie2009elements} we call $X^\top X$
the scatter matrix. To perform regularized GCCA we add $r_jI$ to each
scatter matrix $X_j X_j^\top$ before doing the inversion. Typically
$r_j$ is chosen to be very small e.g. 1e-8.
\begin{eqnarray}
  \widetilde{P}_{j} &=& X_j(X_j^\top X_j+r_jI)^{-1}X_j^\top\\
  M &=& \sum_{j=1}^J \widetilde{P}_{j} \\
  MG &=& G\Lambda \\
  U_j &=& (X_j^\top X_j+r_jI)^{-1} X_j^\top G
\end{eqnarray}


Let $$ A_{j} S_{j} B^\top_{j} \xleftarrow{SVD_{m}} X_j$$
Note that $SVD_m$ denotes a partial SVD where we keep only the top $m$ eigen
vectors of $X_j$. This is required since $X_j$ is a large sparse
matrix and we can not store its complete SVD in memory.
Here $S_j$ is a rectangular diagonal matrix with $m$ non-zero entries, 
and $A_j$ and $B_j$ are orthonormal, unitary matrices.
Then $$\widetilde{P}_j = A_j S_j^\top(r_j I + S_j S_J^\top)^{-1}S_j A_j^\top$$.
Let $T_j \in \mathbb{R}^{m \times m}$ be a diagonal matrix such that
$$T_jT_j^\top = S_j^\top(r_j I + S_j S_J^\top)^{-1}S_j $$ then
$$\widetilde{P}_j = A_j T_j T_j^\top A_j^\top$$
Let
$$\tilde{M} = \left[ A_1T_1 \ldots A_JT_J \right] \in \mathbb{R}^{N
  \times mJ}$$
Then 
$$M = \tilde{M} \tilde{M}^\top$$
Perform a QR decomposition of $\tilde{M}$ to get
$$QR \xleftarrow{QR} \tilde{M}$$ where $$R \in \mathbb{R}^{mJ \times mJ}$$
 then perform eigen decomposition of $RR^\top$ such that
$$ U S U^\top \xleftarrow{eig} RR^\top$$
to get
$$M = Q U S U^\top Q^\top$$
 which implies that
 $$G = QU$$

\subsection{Computing SVD of mean centered $X_j$}
Since $X_j$ are large sparse matrices therefore we cannot explicitly
substract the mean from them. In order to compute the SVD of mean
centered matrices $X_j$ we first compute the partial SVD of uncentered
matrices $\tilde{X}_j$ and then update it using the method of
\cite{brand2006fast}.

%% $$T_j = \sum_{i=1}^m \frac{s_{ji}^2}{s_{ji}^2+r_j} B_{j(:,i)}B_{j(:,i)}^\top$$
%% Where $s_{ji}$ is the ith diagonal element of $S_j$, $B_{j(:,i)}$ is the
%% ith column of $B_{j}$
%% Let $r'_j = diag(R'_j)$, the row vector composed of diagonal elements
%% of $R'_j$. Then the kth column of $T_j$,
%% $$T_{j (:,k)} = B_j (r'_j .\times B_{j(k,:)})^\top$$
%% where $.\times$ denotes element wise multiplication.
%% Since $$M_{(:,k)} = \sum_{j=1}^J T_{j (:,k)}$$, this implies we can
%% compute N columns of M is $$\mathcal{O}(N(J(m+mN)) + J(Nm + m^2))$$
%% Now we use the iterative SVD method of \cite{brand2002incremental} to
%% compute the top $r$ eigen values of $M$

%% We note that since $M$ is
%% symmetric therefore either of the left or the right
%% singular vectors are eigen vector and we don't need to compute
%% both. For sake of speed we do not compute the right
%% singular vectors.

%% \section{Finding SVD for performing GCCA}
%% Now we state the method for incremental SVD which is the same as the
%% original by Brand with one minor trick. The trick is that we
%% precompute the norm of 
%% all the columns of the matrix once before finding the SVD and then
%% arrange the columns and rows in descending order so that the matrix
%% remains symmetric and the increamental SVD algorithm is initialized
%% with the columns having highest norm.

%% Remember that $j$ indexes over language pairs $j \in [1, J]$ and $ X_j \in \mathbb{R}^{N\times d_j}$.
%% Since we cannot store $M$ in memory therefore, we take one pass over
%% the$M$ and calculate $b$ number of columns from left to right at each
%% step. At each step we compute an approximation of the SVD and keep
%% updating it. See Algorithm~\ref{alg:incsvd} for details.

%% \begin{algorithm}[htp]
%%   \label{alg:incsvd}
%%  \KwData{C = A matrix of columns\;
%%    b = Number of columns in C\;
%%    r = the rank that we want to estimate upto\;
%%    $\widetilde{U}, \widetilde{S}$ = The current rank r estimates of
%%    the singular vectors and the singular values.
%%  }
%%  %\KwResult{}
%%  \textbf{Algorithm}\;
%%  C = M[:, 1:b]\;
%%  [$\widetilde{U}, \widetilde{S}$]=SVD(C, r)\;
%%  \For{l = b:b:N}{
%%    C = M[:, l+1:l+b]\;
%%    [$\widetilde{U}, \widetilde{S}$]=incrSVD(C, $\widetilde{U},
%%    \widetilde{S}$, r)\;
%%  }
%%  \textbf{function incrSVD}(C, $\widetilde{U}, \widetilde{S}$, r)
 
 
%%  L = $\widetilde{U}^\top C$ \% Project C onto U \;
%%  H = C - $\widetilde{U}$L\;
%%  Q = $\begin{bmatrix} 
%%    \widetilde{S} & L\\
%%    0 & W \\
%%    \end{bmatrix}$ \;
%%  [$U', S', V'$]=SVD(Q, b)\;
%%  $\widetilde{U'}$ = $[\widetilde{U} J][U'(:, 1:r)]$\;
%%  $\widetilde{S'}$ = S'[1:r]\;
%%  return $[\widetilde{U'}, \widetilde{S'}]$\;
%%  \caption{Incremental SVD}
%% \end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DONE TILL HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\textbf{Training Data}: We trained our embeddings using cooccurence
statistics from a single large bitext corpus that was made by
concatenating Spanish, Czech, French and German portions of the
Europarl v7 bitext corpus \cite{koehn2005europarl}, the $10^9$
French-English corpus \cite{callisonburch2009findings}, The Czech,
German, Spanish and French portions of the News Commentary
\cite{koehn2007experiments}, the United Nations French and Spanish
bitext corpora \cite{eisele2010multiun} and the Chinese and Arabic
Newswire corpora used for the GALE machine translation
project.\footnote{{http://projects.ldc.upenn.edu/gale/data/Catalog.html}}. We
used the berkeley aligner to word align the parallel corpora and then
created cooccurence matrices with rows as english words and columns as
the foreign language word that the english word was aligned
to. Finally we used a total of 6 different languages (French, German,
Czech, Arabic, Spanish and Chinese). The amount of data varied from
language to language as shown in Table~\ref{tab:dataperlang}. We also
used the English portion of the bitext corpora to gather cooccurence
statistics with a symmetric window of size 3. Here the context was the
bigram composed of the left and right words that a word
appeared with.

\textbf{:NOTE: 0 We note that corpus size and corpus type can make a huge
  difference to the quality of embeddings therefore we would
  experiment with different types and sizes of corpora as well}

\begin{table}[htbp]
  \centering
  \begin{tabular}{ccc}
    Language & Sentences & English Tokens \\
    Arabic   & 8,860,048   & 190,796,525  \\
    Czech    & 726,804     & 17,254,309   \\
    German   & 1,870,847   & 44,640,847   \\
    Spanish  & 11,097,173  & 241,454,907  \\
    French   & 30,945,093  & 671,382,577  \\
    Chinese  & 10,290,031  & 215,120,478  \\
    Total    & 63,789,996  & 1,380,649,643  \\
  \end{tabular}  
  \caption{Data used to create GCCA embeddings.}
  \label{tab:dataperlang}
\end{table}

\textbf{Test Data}: We evaluated the representations on the word
analogy task presented in \cite{mikolov2013distributed} and the
following word similarity datasets: 
MEN~\cite{bruni2012distributional},
MTURK~\cite{Radinsky2011word},MC~\cite{miller1991contextual},
RG~\cite{Rubenstein1965Contextual}, SCWS~\cite{Huang2012Improving},
WS\_353~\cite{finkelstein2001placing}, WS\_353\_SEM and WS\_353\_SEM
~\cite{agirre2009study}, RW~\cite{Luong2013morpho}. For the word
similarity tasks we calculate the Spearman and
Pearson correlation and for the Word Analogy task we report
accuracies. 

%% \begin{table}[htbp]
%%   \centering
%%   \begin{tabular}{ccc}
%%     Name  & Description & Citation
%%     TOEFL & 
%%     SCWS  & Word pair similarity in context              & Huang2012Improving
%%     RW    &                                              & Luong2013morpho
%%     MEN   &                                              & bruni2012distributional
%%     MC & 30                                        & miller1991contextual
%%     MTURK & 287                                    & Radinsky2011word
%%     RG & 65                                        & Rubenstein1965Contextual
%%     WS & 353                                        & finkelstein2001placing
%%   \end{tabular}
%%   \caption{caption}
%%   \label{"waiting for reftex-label call..."}
%% \end{table}
\textbf{Hyper parameters and Pre-processing}: Even though CCA is
affine invariant it is still learning a linear transformation
therefore we experiment with non linear preprocessing of the raw
counts. \textbf{:NOTE: 1 Additionally since  we do not
mean normalize before doing CCA therefore practically our procedure
does not remain affine invariant therefore using cooccurence frequency
instead of counts and tfidf, simple ll etc. might make a difference.}
Apart from the identity transform we experiment with logarithmic
transform and power transform
Let C = Observed co-occurrence counts
and F = Observed co-occurrence frequency
and E =  the expected frequency of the word pair.
Then we should try out the following
a. C
b. log(C)
c. C/F
d. log(C)-log(F)
Also we can try to find an appropriate power $\alpha$ to scale the
observed counts C and F.

\textbf{Representations generated}

\textbf{:NOTE: 2 This list describes the types of
  embeddings generated in later tables.}
\begin{itemize}[noitemsep]
\item glove6\_eval\_by\_push : Glove embeddings generated from 6B
  tokens and their evaluation scores as generated by us by 15 August
  2014. 
\item glove42\_eval\_by\_push : Glove embeddings generated from 42B
  tokens and their evaluation scores as generated by us by 15 August
  2014. \textbf{:NOTE: 3 The difference from numbers in square brackets points to a
  bug/misunderstanding in evaluation.}
\item Mikolov\_Full\_Vocab : word2vec embeddings generated from 100B token
  google news corpus.
\item Mikolov\_Pruned : word2vec embeddings which were pruned down by
  types of words (the rows) by interesecting with the vocabulary
  present in the bitext data. This was done so that I could perform
  GCCA. 
\item G\_bitext : The embeddings created by using bitext data alone
  and performing GCCA on it. 
\item U\_bitext : The embeddings created by performing CCA between
  G\_bitext embeddings and Mikolov\_Pruned embeddings and then
  projecting the Mikolov\_Pruned embeddings by using the CCA
  projection matrices. 
\item V\_bitext : The embeddings created by performing CCA between
  G\_bitext embeddings and Mikolov\_Pruned embeddings and then
  projecting the G\_bitext embeddings by using the projection matrices
  created after CCA. 
\item G\_bi+mono : The embeddings created by using bitext data with
  monolingual co-occurence statistics
\item U\_bi+mono : (Same pattern as U\_bitext and G\_bitext)
\item V\_bi+mono : (Same pattern as V\_bitext and G\_bitext)
\item G\_bi+mono+mikolov : Take GCCA over bitext data and monolingual
  text data and mikolov embeddings and then create
  embeddings. \textbf{:NOTE: 4 SINCE WE ARE TAKING GCCA OVER BITEXT DATA AND
    MONOLINGUAL TEXT DATA. IT WOULD BE GREAT IF THE NUMBERS IN
    THIS COLUMN ARE MORE THAN THE NUMBERS IN MIKOLOV\_PRUNED!!!} 
\end{itemize}


\subsection{Results}
The numbers in parenthesis indicate the number of test samples that
were actually used. The rest of the test points were not used because
one of the words was not in our vocabulary.  

In Table~\ref{tab:eval} we see that even though the representation
learnt through GCCA are not better than glove or Mikolov's embeddings,
however taking CCA between representations learnt using GCCA and
Mikolov's representations improves Mikolov's representation. our goal
would be to increase the number of views and to increase the number of
tokens and to directly take GCCA 

\begin{table*}[htbp]
  \centering
  \begin{tabular}{ccc}
Dataset (with recall)                     & glove6\_eval\_by\_pushp & glove42\_eval\_by\_push  \\
JURI                                      &        0.4730           &       0.551353           \\
TOEFL (78 out of 80)                      &        0.8750           &      0.8625              \\
SCWS (2003 out of 2003)                   &        0.5450    [53.9] &      0.5422  [59.6]      \\
RW (2013 out of 2034)                     &        0.3368    [38.1] &      0.3690  [47.8]      \\
MEN (3000 out of 3000)                    &        0.7377    [72.7] &      0.7490  [83.6]      \\
EN\_MC\_30 (30 out of 30)                 &        0.6510           &        0.7761            \\
EN\_MTURK\_287 (287 out of 287)           &        0.6290    [77.8] &        0.6490  [82.9]    \\
EN\_RG\_65 (65 out of 65)                 &        0.7367           &        0.8008            \\
EN\_WS\_353\_ALL (353 out of 353)         &        0.5915    [65.8] &         0.6443  [75.9]   \\
EN\_WS\_353\_REL (252 out of 252)         &        0.5534           &         0.5843           \\
EN\_WS\_353\_SIM (203 out of 203)         &        0.6435           &         0.7089           \\
EN\_TOM\_ICLR13\_SYN (10675 out of 10675) &        0.6715           &         0.6942           \\
EN\_TOM\_ICLR13\_SEM (8869 out of 8869)   &        0.7705           &         0.8102           \\
    
  \end{tabular}
  \caption{caption}
  \label{tab:glove}
\end{table*}


Spearman
\begin{table*}[tp]
  \centering
  \resizebox{2.5\columnwidth}{!}{
  \begin{tabular}{cccccccccccc}
Dataset (with attempts)        & Mikolov\_Full\_Vocab  &Mikolov\_Pruned &G\_bitext    &U\_bitext      &V\_bitext  &G\_bi+mono  &U\_bi+mono  & V\_bi+mono  &G\_bi+mono+mikolov & U\_bi+mono+mikolov &V\_bi+mono+mikolov  \\
JURI                           &        0.6283         &       0.6313   &    0.7090   &     0.6607   &     0.7326 &  0.7132    &   0.6607   &    0.7333   &       0.7264     &   0.6607         &  0.7241               \\
TOEFL (78 out of 80)           &        0.8625 (79)    &       0.8625   &    0.8750   &     0.8750   &     0.9000 &  0.9000    &   0.8750   &    0.9000    &      0.9500     &  0.8750          &  0.9500               \\
SCWS (1978 out of 2003)        &        0.6353 (1979)  &       0.6476   &    0.5896   &     0.6207   &     0.6097 &  0.6011    &   0.6215   &    0.6101    &      0.6054     &  0.6216          &  0.5824               \\
RW (1808 out of 2034)          &        0.4083 (1825)  &       0.4485   &    0.3189   &     0.4780   &     0.3088 &  0.3361    &   0.4780   &    0.3211    &      0.4263     &  0.4780          &  0.4202               \\
MEN (2946 out of 3000)         &        0.7067         &       0.7428   &    0.4810   &     0.7391   &     0.4918 &  0.4869    &   0.7391   &    0.4978    &      0.5134     &  0.7391          &  0.5126               \\
EN\_MC\_30 (30 out of 30)      &          0.7656       &         0.7886 &      0.3591 &       0.7883 &       0.4592 &  0.3862  &     0.7883 &      0.4715  &      0.3905     &  0.7883          &  0.5307               \\
EN\_MTURK\_287 (275 out of 287)&          0.6168       &         0.6327 &      0.4790 &       0.6007 &       0.5218 &  0.4748  &     0.6007 &      0.5367  &      0.4885     &  0.6007          &  0.4241               \\
EN\_RG\_65 (65 out of 65)      &          0.7257       &         0.7607 &      0.5519 &       0.7774 &       0.5847 &  0.5777  &     0.7774 &      0.6241  &      0.5443     &  0.7774          &  0.6249               \\
EN\_WS\_353\_ALL (350 out of 353)       &0.6477        &        0.6833  &     0.5341  &      0.6496  &      0.5494  & 0.5400   &    0.6496  &     0.5338   &      0.5551     &  0.6496          &  0.5368               \\
EN\_WS\_353\_REL (250 out of 252)       & 0.5498       &         0.5996 &      0.4710 &       0.5464 &       0.5300 &  0.4688  &     0.5464 &      0.5020  &      0.4813     &  0.5464          &  0.4979               \\
EN\_WS\_353\_SIM (202 out of 203)        &0.7522       &         0.7783 &      0.5961 &       0.7499 &       0.6235 &  0.6243  &     0.7499 &      0.6284  &      0.6446     &  0.7499          &  0.6064               \\
EN\_TOM\_ICLR13\_SYN (10043 out of 10675)&0.0163       &         0.6336 &      0.3170 &       0.5734 &       0.3265 &  0.3432  &     0.5734 &      0.3474  &      0.3615     &  0.5734          &  0.2987               \\
EN\_TOM\_ICLR13\_SEM (3662 out of 8869)  &0.0046       &         0.1097 &      0.0392 &       0.1110 &       0.0438 &  0.0414  &     0.1110 &      0.0462  &      0.0470     &  0.1110          &  0.0504               \\

  \end{tabular}}
  \caption{caption}
  \label{tab:eval}
\end{table*}



%% Pearson
%% JURI (0 out of 0)                       0.597420
%% JURI (0 out of 0)                       0.663180        0.761930        0.687952        0.773492
%% JURI (0 out of 0)                       0.663180        0.765517        0.687952        0.775001
%% JURI (0 out of 0)                       0.663180        0.773290        0.687952        0.770819
%% JURI (0 out of 0)                       0.499079
%% JURI (0 out of 0)                       0.586661
%% Miko_Full  Miko_pruned       G_bitext       U_bitext       V_bitext         G_bi_mono       U_bi_mono       V_bi_mono       G_bi+mono+miko       U_bi+mono+miko       V_bi+mono+miko
%% 0.862500   0.862500        0.875000        0.875000        0.900000           0.900000        0.875000        0.900000            0.950000        0.875000        0.950000
%% 0.635373   0.647620        0.589620        0.620769        0.609743           0.601182        0.621538        0.610108            0.605402        0.621615        0.582404
%% 0.408386   0.448516        0.318983        0.478068        0.308842           0.336182        0.478068        0.321116            0.426346        0.478068        0.420290
%% 0.706774   0.742855        0.481012        0.739194        0.491814           0.486905        0.739194        0.497840            0.513432        0.739194        0.512681
%% 0.725771   0.760783        0.551999        0.777458        0.584782           0.577767        0.777458        0.624100            0.544306        0.777458        0.624930
%% 0.616830   0.632702        0.479025        0.600778        0.521848           0.474859        0.600778        0.536787            0.488514        0.600778        0.424147
%% 0.765688   0.788607        0.359146        0.788385        0.459279           0.386293        0.788385        0.471518            0.390521        0.788385        0.530708 
%% 0.647728   0.683337        0.534191        0.649617        0.549409           0.540096        0.649617        0.533813            0.555136        0.649617        0.536844
%% 0.549857   0.599603        0.471022        0.546479        0.530096           0.468858        0.546479        0.502031            0.481364        0.546479        0.497938
%% 0.752261   0.778387        0.596106        0.749919        0.623577           0.624321        0.749919        0.628400            0.644622        0.749919        0.606490
%% TOMAS\_ANALOGY
%% 0.016300   0.633630        0.317096        0.573489        0.326557           0.343232        0.573489        0.347447            0.361593        0.573489        0.298735
%% 0.004623   0.109708        0.039238        0.111061        0.043861           0.041493        0.111061        0.046228            0.047018        0.111061        0.050400


\section{Previous Work}
CCA is a specific approach to multi view learning. The general task of
multi view learning is to leverage multiple data sources to expedite
learning and improve performance in tasks. The earliest multi view
learning approaches were algorithms like
"Co-Training"~\cite{blum1998combining},
"CoBoosting"~\cite{collins1999unsupervised} and "2 view
perceptrons"~\cite{brefeld2006efficient}. The applications considered
in these papers were NER and Web Page Type detection but they were
general techniques for labeling sequences. These approaches forced
predictors trained on individual views to agree with each other.  
The following two papers were good overviews of application of CCA for
multi view learning: \cite{kakade2007multi},\cite{ganchevuai08}.

After the publication of the paper \cite{bach2005probabilistic} which gave a probabilistic
interpretation of CCA at least the groups of Dan Klein and Ben Taskar
applied that view of CCA and multi view learning to tasks of Parsing,
MT, NER \cite{ganchevuai08,Burkett2008Two,burkett2010learning,haghighi2008learning}.
%% Amongst these 4 papers there was considerable variation in the
%% objective, whether it used Bhattacharya Distance or Hellinger, does
%% variational inference or uses EM or a directed graphical model
%% versus undirected or used log linear models. 

\cite{faruqui2014improving} used CCA to learn bilingual
representations whereas \cite{dhillon2011multi,dhillon2012two} used
CCA as the primary method to learn vector representations. Their work
is most closely related to ours, since our 
work can be considered a direct generalization of theirs. 

Bilingual text and monolingual text has been used to learn large
collections of paraphrases. \cite{bannard2005paraphrasing} introduced
the trick and 
then \cite{ganitkevitch2013ppdb} built upon it. The reason why these
two views work is 
that they complement each other. Monolingual data can't distinguish
between antonyms but bilingual data can. And bilingual data confounds
words that occur in the same sentence but monolingual data can
distinguish them based on their context. So these are two different
views over the same underlying representation which we can learn by
using CCA or other multi view learning techniques.

Such bilingual data has been used before to learn
representations. For example \cite{zou2013bilingual} used word aligned
parallel corpus to create a Language-to-Language word co-occurence
matrix M and then in a coordinate-ascent hill-climbing fashion first
optimized the english embeddings with respect to chinese embeddings,
and chinese with respect to english.

\section{Future Work}
We are not using larger monolingual datasets as well as other larger
datasets, for example the ClueWeb-FreeBase aligned dataset and other
sources of alignments.

We are also using a small vocabulary size of 100,000 words. Increasing
that would involve scaling up the SVD algorithm to 
larger problems and for that we would have to implement faster methods
of finding SVD like the following.

Also we are using only a basic incremental method of finding SVD and we have not quantified the amount of error that our
method of finding SVD is introducing in the calculations. 
We haven't used newer methods for example Stochastic Optimization for
PCA and PLS [TODO: Add Citation]

Currently some of our results use CCA to fuse the representation
learnt from GCCA and the embeddings learnt by Mikolov et. al. and the
Glove embeddings. We would like to take GCCA over all of them directly.

We have not done Kernel CCA also.

- I guess the time complexity quadratic in n because the matrices are
nxn, right? In other words if you view the columns/rows as an incoming
stream of vectors for incremental SVD, the dimensionality is as large
as the sample size for projection matrices. In that case, we can
consider incremental SVD with missing entries and process partial
columns of the projection matrices. We can discuss that next week.  

- The batch size does not have to be large for the incremental SVD to
converge. That would be make each computation expensive. The point of
stochastic approximation is to make each computation as inexpensive as
possible even if it is a gross approximation and be able to take many
such steps in the same amount of runtime. In fact b=1 should be fine.

The algorithm properly requires SVD of projection matrices
calculated from centered matrices. But I can't explicitly center the
alignment matrices. They would become dense and I would not be able to
store them or to take their SVD. so I am using non-centered
matrices. I have worked out the math to get around this problem but
not implemented it yet.

(The time complexity is,  $t =  (3rnb + nr^2 + nb^2 + nb + (r+b)^3)*n/b + 2 (N(J(m + mN)) + J(Nm + m^2))$;
  The memory complexity is m =  n*r + n*b 
  here r is the final dimension of embeddings = 300
n is the number of words we want embeddings for = 131,133
b is the batch size of the increment. = 7000 ((In general we want to make b as high as possible. To reduce run time and to improve accuracy of incemental SVD)
J is the number of language pairs = 6 )
So the main thing is that we are quadratic in n.  The current method:
would have problems if we also want to get phrase embeddings (we might
be fine if the number of words we align to remains small though, then
we would have to switch to the primal space).


\section{Conclusion}
We have shown that GCCA is a natural framework to fuse data from
multiple sources specifically when all we want is to learn a common
latent representation.

We have shown that Bilingual data is a useful source of information
that should be utilized more heavily.

We have shown that SVD algorithms can be made to scale with some
work. We believe that with more research we would be able to come up
with more user friendly software that could do the job. 

\section*{Acknowledgments}
This material is based on research sponsored DARPA under agreement
number FA8750-13-2-0017 (the DEFT program). We were greatly helped by
the carefully curated list of datasets created by Manaal Faruqui at \url{http://www.cs.cmu.edu/~mfaruqui/suite.html}
We acknolwedge Juri Ganitkevitch for providing the aligned bitext
corpora 
% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{references}
\end{document}
