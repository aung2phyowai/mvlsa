\documentclass[11pt]{article}
\usepackage[ruled,]{algorithm2e}
\usepackage{naaclhlt2015}
\usepackage{times}
\usepackage{mathtools}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{placeins}
\usepackage[normalem]{ulem}
\usepackage[table]{xcolor}
\usepackage{color, colortbl}
\newcommand{\cwindow}{1, 2, 4, 6, 8, 10, 12, 14, 15}
\newcommand{\cwinlen}{9}
\newcommand{\ctotalview}{16}
\newcommand{\xline}[0]{\noindent\underline{\makebox[0.1cm][l]{}}}
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\newcommand{\mb}[1]{\textbf{#1}}
\newcommand{\mi}[1]{\textbf{#1}}
\newcommand{\myu}[1]{\uline{#1}}
\usepackage{array}
\usepackage{enumitem}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\newcolumntype{S}{l}
\makeatletter
\newcommand{\remove}[1]{}
\newcommand{\removet}[1]{#1}
\newcommand*{\@rowstyle}{}

\newcommand*{\rowstyle}[1]{% sets the style of the next row
  \gdef\@rowstyle{#1}%
  \@rowstyle\ignorespaces%
}

\newcolumntype{=}{% resets the row style
  >{\gdef\@rowstyle{}}%
}

\newcolumntype{+}{% adds the current row style to the next column
  >{\@rowstyle}%
}

\newcommand{\raman}[1]{ (\textcolor{red}{Raman: #1})}

\definecolor{lightgray}{gray}{0.96}
\definecolor{darkgray}{gray}{0.7}
\definecolor{darkergray}{gray}{0.0}
%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Multiview LSA: Representation Learning via Generalized CCA}

% \author{Pushpendre Rastogi \\
%   Johns Hopkins University \\
%   {\tt pushpendre@jhu.edu} 
% }

\date{} 

\begin{document}
\maketitle
\begin{abstract}
  Latent Semantic Analysis is a long-standing method for inducing
  vector-based lexical representations from corpus-statistics.  We
  demonstrate that the framework of Generalized Canonical Correlation
  Analysis allows for similar intuitions to LSA, but supports the
  fusion of arbitrary additional perspectives: we term this Multiview
  LSA.  Experiments across a comprehensive collection of datasets show
  this approach to be competitive with the state-of-art, and allows
  for treating representations learned via other mechanisms as simply
  additional informative views in our process.
  % We present the results  of our experiments with
  % the framework of Generalized Canonical Correlation Analysis (GCCA) for extracting vector
  % representations from multiple views such as cooccurrence statistics and
  % even other representations. 
  % The aim of the paper is to analyze the performance of GCCA in fusing
  % multiple sources of linguistic 
  % data and to compare the performance of representations learnt using
  % this technique with  the state of the art.
\end{abstract}

\section{Introduction}

\newcite{winograd1972understanding} wrote that: \emph{``Two sentences
  are paraphrases if they produce the same representation in the
  internal formalism for meaning''}.  This intuition is made soft in
vector-space models \cite{turney2010frequency}, where we say that
expressions in language are of similar meaning, or paraphrastic, if
their representations are \emph{close} under some distance measure.
These vector representations are typically created via some form of
dimensionality reduction over corpus co-occurrence statistics.

One of the earliest and most well-known of these methods is Latent
Semantic Analysis (LSA), where term-paragraph, or term-document
statistics are collected into a matrix, with the most important
eigenvectors under a Singular Value Decomposition (SVD) then forming
the basis in vector-space for representing the input terms
\cite{landauer1997solution}.  LSA has been used widely in fields such
as Information Retrieval and Cognitive Science, but is limited in its
reliance on a single matrix, or \emph{view} of term co-occurrences.
Many recent efforts since the introduction of LSA are similarly
restricted to a single view of terms, focusing on some combination of
finding an ``optimal'' view, paired with more advanced algorithms\raman{Need references}.

Here we address the single-view limitation of LSA by demonstrating
that the framework of Generalized Canonical Correlation Analysis
(GCCA) allows for what we call ``multi-view'' LSA (MVLSA).  This
approach allows for the use of an arbitrary number of views in the
induction process, including those single-view vector-representations
induced using recent alternatives to LSA.

Our experiments show this approach to be competitive to recent
approaches, while being based fundamentally on a decades-old mechanism
widely recognized in the community.  Finally, as a methodological
aside, we discuss the (in-)appropriateness of various datasets being
used as the basis for comparison within the community.

% leading to the
% intuitive conclusion that overly small evaluation sets can lead to
% potentially misleading results.

\section{LSA}

Pushpendre: what is LSA?  You can't call your method Multiview LSA
without first telling the reader what LSA is!


\section{GCCA}
\label{sec:gcca}
%% Let us briefly mention Canonical Correlation analysis to better motivate
%% GCCA. CCA is a procedure for finding subspaces such that once we
%% project two datasets X and Y to those subspaces then the correlation
%% between corresponding projections gets
%% maximized\cite{hotelling1935the}.
%% It can be proven that this happens
%% if and only if the dimensions of the projections are orthogonal and the optimization can
%% be done by constraining the projections to be
%% orthogonal. 
\textbf{GCCA} is an extension of CCA to the case when we have aligned
data from $J$ vector valued random variables.\footnote{See
  \cite{velden2011on} for a proof that GCCA with two views gives the
  same solution as CCA.}  Let $X_j \in \mathbb{R}^{N\times d_j} \;
\forall j \in [1,\ldots,J]$ be the mean centered matrix containing
data from view $j$. Let the number of words in the vocabulary be $N$
and number of contexts (columns in $X_j$) be $d_j$. Note that $N$
remains the same and $d_j$ varies across views. Following standard
notation \cite{hastie2009elements} we call $X_j^\top X_j$ the scatter
matrix and $X_j (X_j^\top X_j)^{-1}X_j^\top$ the projection matrix.

As discussed in \S~\ref{sec:motivation} there are different
characterizations of GCCA\footnote{See \cite{kettenring1971canonical}
  for further details.} and we would perform ``MAX-VAR'' GCCA and find
a matrix $G \in \mathbb{R}^{N\times r}$ and matrices $U_j \in
\mathbb{R}^{d_j \times r}$ that satisfy expression~\ref{eq:gcca}:
\begin{equation}
  \label{eq:gcca}
\begin{split}
  \operatorname*{\arg\,\min}_{G,U_j} & \sum_{j=1}^J \begin{Vmatrix} G - X_jU_j \end{Vmatrix}^2_F \\
  \text{such that } & G^\top G = I
\end{split}
\end{equation}
The matrix $G$ that satisfies expression~\ref{eq:gcca} would also be our
distributed\raman{Why distributed?} representation of the vocabulary.
The optimal value of $G$ is has the following closed form\raman{Not really}:
Finding representations $G$ reduces to spectral decomposition of sum of projection matrices of different views: Define
\begin{align}
P_j =& X_j(X_j^\top X_j)^{-1}X_j^\top \label{eq:pp}\\
M =& \sum_{j=1}^J P_j \label{eq:mm}\\
\end{align}
Then $G$ 


\begin{align}
P_j =& X_j(X_j^\top X_j)^{-1}X_j^\top \label{eq:pp}\\
M =& \sum_{j=1}^J P_j \label{eq:mm}\\
M G =& G \Lambda\\
U_j =& \left(X_j^\top X_j\right)^{-1} X_j^\top G
\end{align}

The above expressions tell us that our word representations are the
eigenvectors of the sum of $J$ projection matrices. Also note that we
explicitly constrained the dimensions of $G$ to be orthogonal to each
other. Orthogonality in representations is a nice property that we
will discuss later.

We can immediately see that we can not compute
 $P_j \in \mathbb{R}^{N \times N}$ because of memory constraints.
%% Since eigenvalues of
%% projection matrices are either 0 or 1 that implies 
%% that eigenvalues of sum of M must lie between 0 and J.
%% This leads us to the problem of finding the eigenvectors of large 
%% projection matrices which have wide and gentle spectrum.
Also the
scatter matrices may be non-singular so the procedure may become
ill-posed. Next we make a single reasonable assumption and present an
approximate regularized method 
to compute GCCA. Parts of the
following method were proposed to us by \cite{savostyanov}.

\noindent\textbf{Approximate Regularized GCCA}: GCCA is typically regularized by adding $r_jI$ to each
scatter matrix $X_j^\top X_j$ before doing the inversion. Typically
$r_j$ is a small constant like 1e-8. Equations~\ref{eq:pp}
and \ref{eq:mm} change to
\begin{align}
  \widetilde{P}_{j} =& X_j(X_j^\top X_j+r_jI)^{-1}X_j^\top \label{eq:6}\\
  M =& \sum_{j=1}^J \widetilde{P}_{j} \label{eq:mmm}
\end{align}

We can side step the computational difficulties as follows.
Let $SVD_m$ denotes a partial SVD where $S_j$ is a rectangular diagonal
matrix that contains only the $m$ largest eigen values and $A_j, B_j$
are square, orthonormal, unitary matrices. Defining $SVD_m$ like this
ensures correctness but in practice we only need to compute $m$
columns of $A_j$. Take the SVD of $X_j$.
$$A_{j} S_{j} B^\top_{j} \xleftarrow{SVD_{m}} X_j$$
 Substitute the above in equation~\ref{eq:6} to get 
$$\widetilde{P}_j = A_j S_j^\top(r_j I + S_j S_J^\top)^{-1}S_j A_j^\top$$ 
Define, $T_j \in \mathbb{R}^{m \times m}$ to be the diagonal matrix such that
$T_jT_j^\top = S_j^\top(r_j I + S_j S_J^\top)^{-1}S_j $ then
$$\widetilde{P}_j = A_j T_j T_j^\top A_j^\top$$
Define, $\tilde{M} = \left[ A_1T_1 \ldots A_JT_J \right] \in \mathbb{R}^{N
  \times mJ}$
Then 
$$M = \tilde{M} \tilde{M}^\top$$
Perform QR decomposition of $\tilde{M}$ to get
$$M = Q R R^\top Q$$
Do eigen decomposition of $R R^\top \in \mathbb{R}^{mJ \times mJ}$
to get its eigen vectors $U$ and eigen values $S$.
$$M = Q U S U^\top Q^\top$$
 which implies $G = QU$. 

\subsection{Computing SVD of mean centered $X_j$}
\label{ssec:svdmc}
Recall that we assumed $X_j$ to be mean centered matrices. Let $Z_j
\in \mathbb{R}^{N \times d_j}$ be sparse matrices containing
mean-uncentered cooccurrence counts. Let $f_j = n_j \circ t_j $ be the preprocessing
function that we apply to $Z_j$. 
\begin{align}
  Y_j =& f_j (Z_j) \\
  X_j =& Y_j + 1 (1^\top Y_j)
\end{align}
In order to compute the SVD of mean centered matrices $X_j$ we first
compute the partial SVD of uncentered 
matrix $Y_j$ and then update it. See \cite{brand2006fast} for details.
%% We experimented with representations created from the
%% uncentered matrices $Y_j$ and found that they performed as well as 
%% the mean centered versions but we would not mention them further since
%% it is computationally efficient to follow the principled approach. We
%% note, however, that even the method of mean-centering the SVD
%% produces an approximation.

\subsection{Handling missing rows across views}
\label{ssec:missing}
%% Recall that we assumed that rows of $X_j \forall j \in [0,\ldots , J]$ correspond to unique
%% words in the vocabulary and that the rows correspond to each
%% other.
With real data it may happen that a word was not observed in a view at
all. A significant number of 
missing rows can corrupt the learnt representations since the rows
in the left singular matrix become zero.
%% The procedure described above
%% can not recover from this and the representation for those words may become a
%% one hot vector. 
To counter this problem we adopted a variant of the ``missing-data
passive'' algorithm from \cite{van2006generalized} who modified the
GCCA objective to counter the problem of missing 
rows.\footnote{A more recent paper, \cite{van2012generalized},
  describes newer iterative and non-iterative(Test-Equating Method)
  approaches for handling missing values. It is possible that using
  one of those methods could improve performance.}
Specifically, the objective now becomes:
\begin{equation}
  \label{eq:gcca2}
\begin{split}
  \operatorname*{arg\,min}_{G,U_j} & \sum_{j=1}^J \begin{Vmatrix} K_j(G - X_jU_j) \end{Vmatrix}^2_F \\
  \text{such that } & G^\top G = I
\end{split}
\end{equation}
if row $i$ of view $j$ is observed then $[k_j]_{ii} = 1$ otherwise $0$.
Essentially $K_j$ is a diagonal row-selection matrix which ensures
that we optimize our representations only on the observed rows. Note that
$X_j = K_jX_j$ since the rows that $K_j$ removed were already
zero. Let, $K =
\sum_j K_j$ then the optima
of the objective can be computed by modifying Equation~\ref{eq:mmm} as:
\begin{align}
  M =& K^{-\frac{1}{2}}(\sum_{j=1}^J P_j)K^{-\frac{1}{2}}
\end{align}
Again if we regularize and approximate the GCCA solution, we get
$G=QU$ where $Q, R$ come from the QR decomposition of
$K^{-\frac{1}{2}}\tilde{M}$. Also, we mean center the matrices using
only the observed rows.

Also note that other heuristic weighting schemes could be used
here. For example if we modify our objective as follows then we would
recover the objective of \cite{pennington2014glove}:
\begin{equation}
  \label{eq:gcca3}
\begin{split}
  \operatorname*{arg\,min}_{G,U_j} & \sum_{j=1}^J \begin{Vmatrix} W_j K_j(G - X_jU_j) \end{Vmatrix}^2_F \\
  \text{such that } G^\top G &= I \\
  \text{where } [W_j]_{ii} &= \frac{w_i}{w_{\max}}^{\frac{3}{4}} \text{ if } w_i <
  w_{\max} \text{ else } 1 \\
  \text{and } w_i &=  \sum_k [X_j]_{ik}
\end{split}
\end{equation}


\section{Motivation}
\label{sec:motivation}

If our goal is to find vector space representations or
\emph{embeddings} of words from multiple sources of data, then let us
enumerate the interpretations of GCCA to motivate why applying this
framework to our problem makes sense.

\begin{enumerate}[leftmargin=*]
\item \cite{horst1961generalized} provided the following
  interpretation. Assume that you have samples of $m$
  ``co-variates''; Then GCCA finds unit variance linear
  projections $Z$ such that some measure of the inter-projection ``correlation
  matrix'' $\Phi$ is maximized. Specifically we choose the measure to be
  the ``spectral-norm'' or the largest singular value of $\Phi$. This
  is referred to as ``MAX-VAR'' GCCA. A more illuminating
  interpretation of maximizing the ``spectral-norm'' of $\Phi$ is that
  by doing so we are finding $Z$ that can be best explained by a rank
  one approximation, in other words we are finding $Z$ that are most
  amenable to rank-one PCA, or that can be best explained by a single
  term factor model.
\item \cite{carroll1968generalization} derived ``MAX-VAR'' GCCA from a
  the following objective. They found an orthogonal
  representation $G$ of the co-variates $Z_j$ that was maximally
  correlated to them. Mathematically they maximized
   the expression $\sum_j \textrm{correlation}(G, Z_j)^2$. In words
   this expression is telling us to find the best middle ground
   representation $G$ between the projections $Z_j$ as measured by the
   norm of the correlation vector $\phi$. This seems like a sensible
   objective for the problem of learning lexical representation from
   multiple arbitary views/sources of data and therefore
   we use it.
\item \cite{bach2005probabilistic} presented a probabilistic
  intepretation for CCA. Though they did not generalize it to
  include GCCA we believe that one could give a probabilistic
  interpretation of ``MAX-VAR'' GCCA easily and we are working on
  it. We mention it, since a probabilistic
  interpretation would allow us to build a generative model and learn
  lexical representations unlike methods like Glove or LSA that rely
  solely on global term cooccurrence matrices and cannot calculate
  perplexity or generate sequences.
\item \cite{via2007learning} presented a neural network model of GCCA and
  adaptive/incremental GCCA. That approach is out of the scope of this work.
\end{enumerate}

Let us make a connection to multi-view learning to further motivate
why using a large number of views can help during learning. The aim of
multi-view learning is to leverage multiple views during training to
improve generalization. GCCA is a general multi view learning method that has the appealing
property that the representations it learns are invariant to affine
transformations of the input, which reduces the number of
pre-processing options. For instance, we don't have to choose 
between log(tf) and log(tf-idf) transformation since log(tf) is the
same as log(tf-idf) up to an affine transformation. Another benefit of
using GCCA over vanilla LSA is that since we can naturally 
fuse the statistics generated by using different window sizes we
don't have to specify an arbitray weighting method, like reciprocal
weighting, for creating a single cooccurrence matrix to represent a
corpus.


%% For example, we can define the rows to be English language words
%% and the columns to be the foreign language words and
%% count the number of times an English word aligns with a
%% foreign language word in a word aligned bitext corpus. Similarly,
%% a monolingual corpus provides multiple sources of 
%% cooccurrence statistics depending on the definition of the
%% context. By changing context of a word from the last word to 15th
%% previous word we can
%% create 15 different sources of non redundant cooccurrence
%% statistics. Even structured databases like WordNet and
%% FrameNet can be used to gather cooccurrence statistics and we would
%% give an example of how FrameNet can be used for this purpose later in
%% the paper. Finally, we can fuse representation generated using
%% multiple different algorithms with the goal that the fused representations should perform better
%% than the components.

In Section~\ref{sec:gcca} we present GCCA and a fast method to
compute it. 
Then we describe our Train and Test data in
Section~\ref{sec:data} followed by Experiments and Results in
Section~\ref{sec:exp} and conclude with Discussion and Future~Work. 
We call this method \textit{Multiview LSA} because we are using GCCA
which uses multiple views and because GCCA is a generalization of PCA and LSA
relies on PCA. 

\section{Data}
\label{sec:data}
\noindent\textbf{Training Data}: We used the English portion of the \textit{Polyglot} wikipedia dataset
released by \cite{al2013polyglot} to create 15 irredundant views of
cooccurrence statistics where element $[z]_{ij}$ of view $Z_k$
represents that number of times word $w_j$ occurred $k$ words behind
$w_i$.
%% We lowercased all the words and discarded all
%% words which were longer than 5 characters and contained more than 3 non
%% alphabetical symbols. This was done to preserves years and smaller
%% numbers.
We selected the top 500K words by occurrence to 
create our vocabulary for the rest of the paper.

We extracted cooccurrence statistics from a large bitext corpus that was made by combining a
number of parallel bilingual corpora. See \cite{ganitkevitch2013ppdb} for details and
Table~\ref{tab:dataperlang} for a summary. The Berkeley aligner was used for word alignment. Element
$[z]_{ij}$ of the \textit{bitext} matrix represents the number of times English
word $w_i$ was aligned to the foreign word $w_j$.

We also used the dependency relations in the \textit{Annotated
  Gigaword Corpus}~\cite{annotatedGigaword12} to create 21
views\footnote{Following is the list of dependency relations that we
  used: nsubj, amod, advmod, rcmod, dobj, prep\xline{}of,
  prep\xline{}in, prep\xline{}to, prep\xline{}on, prep\xline{}for,
  prep\xline{}with, prep\xline{}from, prep\xline{}at, prep\xline{}by,
  prep\xline{}as, prep\xline{}between, xsubj, agent, conj\xline{}and,
  conj\xline{}but, pobj. We selected these dependency relations since
  they seemed to be the particularly interesting which could capture
  different aspects of similarity.}  where element $[z]_{ij}$ of view
$Z_{\textrm{dep}}$ represents the number of times word $w_j$ occurred
as the governor of word $w_i$ with dependency relation $\textrm{dep}$.

We combined the knowledge of paraphrases present in FrameNet and PPDB by
using the dataset created by \newcite{rastogi2014augmenting} to create a
\textit{FrameNet} view. Element $[z]_{ij}$ of the \textit{FrameNet}
view represents whether word $w_i$ was present in frame
$f_j$. Similarly we combined the knowledge of morphology present in
the \textit{CatVar} database released by \newcite{habash2003catvar} and
\textit{morpha, morphg} released by \newcite{minnen2001applied}.
The morphological views and the frame semantic views were especially
sparse with densities of 0.0003\% and 0.03\%. While we could have utilized more sources of semantic data like
Coccurrence in WordNet Synsets or Narrative Chains etc, we decided
that the above set would be representative enough to inform us of the
merits or demerits of the MVLSA method and stopped collecting more data.

\begin{table}[htbp]
  \centering
  \rowcolors{1}{}{lightgray}
  \begin{tabular}{lll}
    Language & Sentences & English Tokens \\
    \hline
    Bitext-Arabic   & 8.8M   & 190M  \\
    Bitext-Czech    & 7.3M   & 17M   \\
    Bitext-German   & 1.8M   & 44M   \\
    Bitext-Spanish  & 11.1M  & 241M  \\
    Bitext-French   & 30.9M  & 671M  \\
    Bitext-Chinese  & 10.3M  & 215M  \\
    Monotext-En-Wiki& 75M    & 1.7B 
  \end{tabular}  
  \caption{Portion of data used to create GCCA representations.}
  \label{tab:dataperlang}
\end{table}

\noindent\textbf{Test Data}: We evaluated the representations on the
word similarity datasets listed in Table~\ref{tab:testlist}. The first
10 datasets in Table~\ref{tab:testlist} were annotated with different
rubrics and rated on different scales. But broadly they all
contain human judgements about how similar two words are.
The ``T-SYN'' and ``T-SEM'' datasets contain 4-tuples of
analogous words and the task is to predict the missing word given the
first three.

\begin{table*}[ht] \label{tab:testlist}
  %\setlength{\tabcolsep}{1pt}
  %\begin{adjustwidth}{0cm}{}
  \rowcolors{1}{}{lightgray}
  \resizebox{\textwidth}{!}{
  \begin{tabular}{l c c c c H H H c H l}
    Acronym & Size  & \specialcell{LC\\(0.01,0.5)} & \specialcell{LC\\(0.01,0.9)} & \specialcell{LC\\(1e-3,0.5)} & \specialcell{LC\\(1e-3,0.9)} & \specialcell{LC\\(0.25,0.7)}& \specialcell{LC\\(0.25,0.5)} & \specialcell{LC\\(0.05,0.5)} & \specialcell{LC\\(0.05,0.9)} & Reference  \\
    \hline
    
    MEN    & 3000  & 4.2  & 1.8  & 5.6  & 2.5  & 0.9 & 1.2 & 3    & 1.3  & \cite{bruni2012distributional}  \\
    RW     & 2034  & 5.1  & 2.3  & 6.8  & 3    & 1.1 & 1.4 & 3.6  & 1.6  & \cite{Luong2013morpho}          \\
    SCWS   & 2003  & 5.1  & 2.3  & 6.8  & 3    & 1.1 & 1.5 & 3.6  & 1.6  & \cite{Huang2012Improving}       \\
    SIMLEX & 999   & 7.3  & 3.2  & 9.7  & 4.3  & 1.6 & 2.1 & 5.2  & 2.3  & \cite{hill2014simlex}           \\
    WS     & 353   & 12.3 & 5.5  & 16.3 & 7.3  & 2.7 & 3.6 & 8.7  & 3.9  & \cite{finkelstein2001placing}   \\
    MTURK  & 287   & 13.7 & 6.1  & 18.1 & 8.1  & 3.1 & 4   & 9.7  & 4.3  & \cite{Radinsky2011word}         \\
    WS-REL & 252   & 14.6 & 6.5  & 19.3 & 8.6  & 3.3 & 4.2 & 10.3 & 4.6  & \cite{agirre2009study}          \\
    WS-SEM & 203   & 16.2 & 7.3  & 21.4 & 9.6  & 3.6 & 4.7 & 11.5 & 5.1  & -Same-As-Above-                 \\
    RG     & 65    & 28.6 & 12.9 & 37.0 & 16.8 & 6.6 & 8.5 & 20.6 & 9.2  & \cite{Rubenstein1965Contextual} \\
    MC     & 30    & 41.7 & 19   & 52.4 & 24.3 & 10  & 13  & 30.6 & 13.8 & \cite{miller1991contextual}     \\ \hline
    T-SYN  & 10675 & 0.95 &      &      &      &     &     & 0.68 &      & \cite{mikolov2013distributed}   \\
    T-SEM  & 8869  & 1.03 &      &      &      &     &     & 0.74 &      & -Same-As-Above-                 \\ \hline
    TOEFL  & 80    & 8.13 &      &      &      &     &     & 6.30 &      & \cite{landauer1997solution}
  \end{tabular}
  }
  \caption{List of test datasets used. The first 10 datasets contain human
    judgements of annotations and we would report Spearman correlation
    of  the human ratings with similarity between the word
    representations.
    T-SYN and T-SEM are open vocabulary tasks and TOEFL is a closed
    vocabulary task and we would report accuracies on
  those tasks with the full vocabulary. The LC column contains the
  least counts for significant difference in performance metrics for
  these datasets. E.g. the value 20.6 in the RG row under the
  LC(0.05, 0.5) column signifies that if the difference between the
  ratings produced by algorithms A and B have spearman correlation
  $\rho_{AB} \le 0.5$ and $\rho_{A,RG} - \rho_{B,RG} \le 20.6$ then there is a
  greater than 5\% chance that the difference would vanish under
  different training conditions.}
  %\end{adjustwidth}
\end{table*}

While surveying the literature we found that even for small test sets researchers only report the Spearman Correlations e.g. \cite{hill2014not,faruqui2014improving,faruqui2014retrofitting} between the similarity values assigned by the embeddings produced by their method and the
average human ratings. \newcite{steiger1980tests} proposed a test for finding the significance of difference in spearman correlations which needs the spearman correlation $\rho_{AB}$ between ratings of the two algorithms. Since one usually doesn't have access to
those scores  we experimented over a range of fixed values for those
scores and found the ``least count (LC)'' of difference in correlation coefficients that would allow us to say with
some confidence that the correlations produced by two algorithms are
truly different. The LC is the smallest number that 
when added to $r_{AT} \in (0, 1)$ lowers the probability of null
hypothesis to acceptable levels.

For finding the LC for the last three accuracy based tests we
used the beta-binomial setup.  We estimated posterior
probability of $\theta_1, \theta_2$ given a $\beta(1,1)$ prior and
given the data. Then we computed $\tilde{p} = p(\theta_2 - \theta_1 >
0 | \hat{\theta_1}, \hat{\theta_1} + \textrm{LC})$ and chose
the smallest value of LC for which $\tilde{p} > 0.95$.

We hope that
the high values of LC for the small sized datasets would convince
researchers to either include confidence measures with their reported
performance metrics or discourage the usage of those datasets.

Unfortunately, there are no widely followed train-test splits of the above
datasets and we also evaluated the effects of hyper-parameter tuning
on the entire test set therefore our final comparison could have
favored us due to ``soft supervision'' on these datasets while
hyperparameter tuning. However the consistent performance of our
method across the test sets lends hope that the trends we report would
generalize. 

\section{Experiments and Results}
\label{sec:exp}
Let us first enumerate the central questions which we wanted to answer through
our experiments: 
\begin{enumerate}[leftmargin=*]
  \itemsep-0.1em 
\item How do hyper parameters affect performance ?
%\item How much does the performance vary on different test sets and why ?
\item What is the contribution of the multiple sources of data to performance ?
\item How does the performance of MVLSA compare with other methods ?
\end{enumerate}

\noindent\textbf{Effect of Hyper parameters}: 
$f_j$: We modeled the preprocessing function $f_j$ as the composition
of two functions, i.e. $f_j = n_j \circ t_j$.
  $n_j$ represents nonlinear preprocessing that is usually
  employed with LSA. We experimented by setting $n_j$ to be
  Identity, logarithm of count plus one and the Fourth root of the
  count.
  %% \footnote{We also experimented with other powers of the counts (0.12, 0.5
  %% and 0.75) on a smaller dataset and found that the fourth root
  %% performed the best.}
  $t_j$ represents the truncation of columns and can be interpreted as
  a type of regularization of the raw counts themselves through which
  we prune away the noisy contexts. Decrease in $t_j$
  also reduces the influence of views that have a large number of
  context columns and emphasizes the sparser views. 
  Table~\ref{tab:n} and Table~\ref{tab:t} show the results.
\begin{table}[htbp]
  \begin{tabular}{=l| +c +c +c}
    Test Set                            & Log  & Count & Count$^{\frac{1}{4}}$ \\ \hline
    MEN                                 & 67.5 & 59.7  & \mb{70.7}                  \\
    RW                                  & 31.1 & 25.3  & \mb{37.8}                  \\
    SCWS                                & 64.2 & 58.2  & \mb{66.6}                  \\\remove{
    SIMLEX                              & 36.7 & 27.0  & \mb{38.0}                  \\
\rowstyle{\color{darkergray}}    WS     & 68.0 & 60.4  & \mb{70.5}                  \\
\rowstyle{\color{darkergray}}    MTURK  & 57.3 & 55.2  & \mb{60.8}                  \\
\rowstyle{\color{darkergray}}    WS-REL & 60.4 & 52.7  & \mb{62.9}                  \\
\rowstyle{\color{darkergray}}    WS-SEM & 75.0 & 67.2  & \mb{76.2}                  \\
\rowstyle{\color{darkergray}}    RG     & 69.1 & 55.3  & \mb{75.9}                  \\
\rowstyle{\color{darkergray}}    MC     & 70.5 & 67.6  & \mb{80.9}                  \\}
    T-SYN                               & 45.7 & 21.1  & \mb{53.6}                  \\
    T-SEM                               & 25.4 & 15.9  & \mb{38.7}                  \\\remove{
  \rowstyle{\color{darkergray}}  TOEFL  & 81.2 & 70.0  & \mb{81.2} }
  \end{tabular}
  \caption{Performance versus $n_j$, the non linear processing of
    Co-occurrence counts.$\, t =200K, \; m=500, \; v=16, \; k=300$.}
  \label{tab:n}
\end{table}

\begin{table}[htbp]
  \resizebox{0.5\textwidth}{!}{
  \begin{tabular}{=l | +c +c +c +c H +c H +c}
Test Set                            & 6.25K & 12.5K & 25K  & 50K  & 75K  & 100K & 150K & 200K \\ \hline
MEN                                 & 70.2  & \mi{71.2}  & \mi{71.5} & \mi{71.6} & \mi{71.4} & \mi{71.2} & \mi{71.0} & \mi{70.7} \\   
RW                                  & \mi{41.8}  & \mi{41.7}  & \mi{41.5} & \mi{40.9} & \mi{40.7} & 39.6 & 38.3 & 37.8 \\ 
SCWS                                & \mi{67.1}  & \mi{67.3}  & \mi{67.1} & \mi{67.0} & \mi{67.3} & \mi{66.9} & \mi{66.8} & \mi{66.6} \\ \remove{
SIMLEX                              & 42.7  & \mb{42.4}  & 41.9 & 41.3 & 40.5 & 39.5 & 38.4 & 38.0 \\ 
\rowstyle{\color{darkergray}}WS     & 68.1  & 70.8  & 71.6 & 71.2 & 71.3 & 70.2 & 70.8 & 70.5 \\ 
\rowstyle{\color{darkergray}}MTURK  & 62.5  & 59.7  & 59.2 & 58.6 & 58.3 & 60.3 & 61.0 & 60.8 \\ 
\rowstyle{\color{darkergray}}WS-REL & 60.8  & 65.1  & 65.7 & 64.8 & 65.2 & 63.7 & 63.7 & 62.9 \\ 
\rowstyle{\color{darkergray}}WS-SEM & 77.8  & 78.8  & 78.8 & 78.2 & 77.7 & 76.5 & 77.0 & 76.2 \\ 
\rowstyle{\color{darkergray}}RG     & 72.7  & 74.4  & 74.7 & 75.0 & 75.0 & 74.3 & 75.6 & 75.9 \\ 
\rowstyle{\color{darkergray}}MC     & 75.2  & 75.9  & 79.9 & 80.3 & 81.0 & 76.9 & 79.6 & 80.9 \\}
T-SYN                               & 59.2  & \mi{60.0}  & \mi{59.5} & 58.4 & 57.4 & 56.1 & 54.3 & 53.6 \\
T-SEM                               & 37.7  & \mi{38.6}  & \mi{39.4} & \mi{39.2} & \mi{39.4} & 38.4 & \mi{38.8} & \mi{38.7} \\\remove{
\rowstyle{\color{darkergray}}TOEFL  & 88.8  & 87.5  & 85.0 & 83.8 & 83.8 & 83.8 & 82.5 & 81.2}
      \end{tabular}
  }
  \caption{Performance versus the truncation threshold, $t$, of raw
    cooccurrence counts. We used $n_j=\textrm{Count}^{\frac{1}{4}}$
    and other settings were the same as Table~\ref{tab:n}.} 
  \label{tab:t}
\end{table}
$m$: The number of left singular vectors extracted after SVD of the preprocessed cooccurrence
  matrices can again be interpreted as a type of regularization, since
  the result of this truncation is that we find cooccurrence patterns 
  only between the top left singular vectors. We set $m_j = max(d_j,
  m)$ with $m=[100, 300, 500]$. See table~\ref{tab:n}.

\begin{table}[htbp]
  \begin{tabular}{=l | +c +c +c +c}
Test Set                            & 100  & 200  & 300  & 500  \\\hline
MEN                                 & 65.6 & 68.5 & \mi{70.1} & \mi{71.1} \\
RW                                  & 34.6 & \mi{36.0} & \mi{37.2} & \mi{37.1} \\
SCWS                                & 64.2 & \mi{65.4} & \mi{66.4} & \mi{66.5} \\\remove{
SIMLEX                              & 38.4 & 40.6 & \mb{41.1} & 40.3 \\
\rowstyle{\color{darkergray}}WS     & 60.4 & 67.1 & 69.4 & \mb{71.1} \\
\rowstyle{\color{darkergray}}MTURK  & 51.3 & 58.3 & 58.4 & \mb{58.9} \\
\rowstyle{\color{darkergray}}WS-REL & 49.0 & 58.2 & 61.6 & \mb{65.1} \\
\rowstyle{\color{darkergray}}WS-SEM & 73.6 & 76.8 & 76.8 & \mb{78.0} \\
\rowstyle{\color{darkergray}}RG     & 61.6 & 69.7 & 73.2 & \mb{74.6} \\
\rowstyle{\color{darkergray}}MC     & 65.6 & 74.1 & \mb{78.3} & 77.7 \\}
T-SYN                               & 50.5 & \mi{56.2} & \mi{56.4} & \mb{56.4} \\
T-SEM                               & 24.3 & 31.4 & 34.3 & \mb{40.6} \\\remove{
\rowstyle{\color{darkergray}} TOEFL & 80.0 & 81.2 & \mb{82.5} & 80.0}
  \end{tabular}                                        
  \caption{Performance versus $m$, the number of left     
singular vectors extracted from raw cooccurrence counts. We set
$n_j=\textrm{Count}^\frac{1}{4}, \; t=100K, \; v=25, \;
k=300$.} 
  \label{tab:m}
\end{table}

$k$: Table~\ref{tab:k} demonstrates the variation in performance
versus the dimensionsionality of the learnt vector representations of the
  words. Since the dimensions of the MVLSA representations are
  orthogonal to each other therefore creating lower dimensional
  representations is a trivial matrix slicing operation and does not
  require retraining.
  \begin{table}[htbp]
  \begin{tabular}{=l | +c H +c +c +c +c +c}
Test Set                            & 10   & 25   & 50   & 100  & 200       & 300       & 500       \\\hline
MEN                                 & 49.0 & 59.3 & 67.0 & \mb{69.7} & \mb{70.2} & \mi{70.1} & \mb{69.8}\\
RW                                  & 28.8 & 33.1 & 33.3 & 35.0 & 35.2      & \mb{37.2} & \mi{38.3} \\
SCWS                                & 57.8 & 62.8 & 64.4 & \mi{65.2} & \mi{66.1}      & \mb{66.4} & \mi{65.1}      \\\remove{
SIMLEX                              & 24.0 & 30.1 & 33.9 & 36.1 & 38.9      & 41.1      & \mb{42.0} \\
\rowstyle{\color{darkergray}}WS     & 46.8 & 57.5 & 63.4 & 69.5 & 69.5      & 69.4      & 66.0      \\
\rowstyle{\color{darkergray}}MTURK  & 54.6 & 65.9 & 67.7 & 61.6 & 60.5      & 58.4      & 57.4      \\
\rowstyle{\color{darkergray}}WS-REL & 38.4 & 49.5 & 55.8 & 63.1 & 62.4      & 61.6      & 56.3      \\
\rowstyle{\color{darkergray}}WS-SEM & 55.3 & 64.7 & 69.9 & 76.9 & 77.1      & 76.8      & 75.6      \\
\rowstyle{\color{darkergray}}RG     & 48.8 & 60.5 & 66.1 & 69.7 & 75.1      & 73.2      & 72.5      \\
\rowstyle{\color{darkergray}}MC     & 37.0 & 57.5 & 59.0 & 71.3 & 79.1      & 78.3      & 75.7      \\}
T-SYN                               & 9.0  & 28.4 & 41.2 & 52.2 & 55.4      & \mb{56.4} & 54.4      \\
T-SEM                               & 2.5  & 10.8 & 21.8 & 34.8 & \mb{35.8} & 34.3      & 33.8      \\\remove{
\rowstyle{\color{darkergray}} TOEFL & 57.5 & 73.8 & 72.5 & 76.2 & 81.2      & 82.5      & 85.0}
  \end{tabular}
  \caption{Performance versus $k$, the final dimensionality of the
    embeddings. We set $ m=300$ and other settings were same as Table~\ref{tab:m}.}
  
  \label{tab:k}
\end{table}

$v$: Recall that in Expression~\ref{eq:gcca3} we described a method to
  set $W_j$. We experimented with a different, more global, heuristic to
  set $[W_j]_{ii} = (K_{ww} \ge v)$. Essentially we removed all
  words that did not appear in $v$ views before doing
  GCCA. Table~\ref{tab:v} shows that changes in $v$ are largely
  inconsequential for performance. In abscence of clear evidence in favor of regularization we
  decided to regularize as little as possible and chose $v=16$. We were
  not able to set $v=0$ since our current implementation of GCCA
  procedure requires us to load the entire $\tilde{M}$ matrix in
  memory which is 45*500*500K*8 Bytes = 90GB.
  %% While this is certainly
  %% possible with current hardware a more memory efficient approach is
  %% desirable.
  This is a drawback of our current implementation and we
  are working on implementing a more scalable version of this
  algorithm so that we can run experiments with larger
  vocabularies. For our current experiments the largest vocabulary we
  used consisted of 361K words and had 100\% recall on all the test
  sets except for RW on which the recall was 92\%.
  %% Also note that though the
  %% size of $G$ decreases as we increase $v$ (and hence the recall of
  %% our embeddings) for these experiments the recall of the vocabulary
  %% over the test sets remained complete except for RW and T-SEM for
  %% which the recall was 1700 out of 2034 and 8714 out of 8869
  %% respectively.
  
  \begin{table}[htbp]
  \begin{tabular}{=l | +c +c H +c H +c H +c}
Test Set                            & 16   & 17   & 19   & 21   & 23   & 25   & 27   & 29   \\ \hline
MEN                                 & \mb{70.4} & \mb{70.4} & \mi{70.2} & \mi{70.2} & \mi{70.1} & \mi{70.1} & \mi{70.0} & \mi{70.0} \\
RW                                  & \mb{39.9} & \mi{38.8} & \mi{40.1} & \mi{39.7} & 38.3 & 37.2 & 35.3 & 33.5 \\
SCWS                                & \mb{67.0} & \mb{66.8} & \mb{66.8} & \mb{66.5} & \mb{66.3} & \mb{66.4} & \mb{66.1} & \mb{65.7} \\\remove{
SIMLEX                              & 40.7 & 41.0 & 41.1 & \mb{41.2} & 41.2 & 41.1 & 41.1 & 41.0 \\
\rowstyle{\color{darkergray}}WS     & 69.5 & 69.4 & 69.5 & 69.5 & 69.4 & 69.4 & 69.3 & 69.1 \\
\rowstyle{\color{darkergray}}MTURK  & 59.4 & 59.2 & 59.3 & 59.2 & 58.7 & 58.4 & 58.0 & 58.0 \\
\rowstyle{\color{darkergray}}WS-REL & 62.1 & 61.9 & 62.1 & 62.3 & 61.9 & 61.6 & 61.4 & 61.1 \\
\rowstyle{\color{darkergray}}WS-SEM & 76.8 & 76.8 & 76.9 & 77.0 & 76.7 & 76.8 & 76.7 & 76.8 \\
\rowstyle{\color{darkergray}}RG     & 73.0 & 72.8 & 72.7 & 72.8 & 73.6 & 73.2 & 73.4 & 73.7 \\
\rowstyle{\color{darkergray}}MC     & 75.0 & 76.0 & 76.4 & 76.5 & 78.2 & 78.3 & 78.6 & 78.6 \\}
T-SYN                               & \mb{56.0} & \mb{55.8} & \mb{56.0} & \mb{55.9} & \mb{56.3} & \mb{56.4} & \mb{56.3} & \mb{56.0} \\
T-SEM                               & \mb{34.6} & \mb{34.3} & \mb{34.1} & \mb{34.0} & \mb{34.5} & \mb{34.3} & \mb{34.4} & \mb{34.3} \\\remove{
\rowstyle{\color{darkergray}} TOEFL & 85.0 & 85.0 & 85.0 & 83.8 & 83.8 & 82.5 & 82.5 & 80.0}
    \end{tabular}
  \caption{Performance versus minimum view support threshold $v$, The other
      hyperparameters were $n_j=\textrm{Count}^{\frac{1}{4}}, \;
      m=300, \; t=100K$. Though a clear best setting did not emerge,
      we chose $v=25$ as the middle ground.}
  \label{tab:v}
\end{table}
  
$r_j$: The regularization parameter ensures that all the
  inverses exist at all points in our method. We found that the
  performance of our  procedure was invariant to $r$ over a large
  range from 1 to 1e-10. This was because even the 1000th singular
  value of  our data was much higher than 1 which is
  consistent with the observation that cooccurrence datasets in NLP
  tend to have gently sloping spectrum. For all the experiments in
  this paper we set $r=1e-5$.


\begin{table*}[ht]
  \label{tab:j}
   \setlength\tabcolsep{3pt}
  \begin{tabular}{=l| +c +c +c +c +c +c +c +c}
Test Set              & \specialcell{All\\Views} & !Framenet &
!Morphology & !Bitext & !Wikipedia & !Dependency &
\specialcell{!Morphology\\!Framenet} &
\specialcell{!Morphology\\!Framenet\\!Bitext} \\\hline
MEN                                 & \mb{70.1} & \mi{69.8} & \mi{70.1} & \mi{69.9} & 46.4 & 68.4 & \mi{69.5} & 68.4 \\
RW                                  & \mb{37.2} & \mi{36.4} & \mi{36.1} & 32.2 & 11.6 & 34.9 & 34.1 & 27.1 \\
SCWS                                & \mb{66.4} & \mi{65.8} & \mi{66.3} & 64.2 & 54.5 & \mi{65.5} & \mi{65.2} & 60.8 \\\remove{
SIMLEX                              & 41.1 & 40.1 & 41.1 & 37.8 & 32.4 & \mb{44.1} & 38.9 & 34.4 \\
\rowstyle{\color{darkergray}}WS     & 69.4 & 69.1 & 69.2 & 67.6 & 43.1 & 70.5 & 69.3 & 66.6 \\
\rowstyle{\color{darkergray}}MTURK  & 58.4 & 58.3 & 58.6 & 55.9 & 52.7 & 59.8 & 57.9 & 55.3 \\
\rowstyle{\color{darkergray}}WS-REL & 61.6 & 61.5 & 61.4 & 59.4 & 38.2 & 63.5 & 62.5 & 58.8 \\
\rowstyle{\color{darkergray}}WS-SEM & 76.8 & 76.3 & 76.7 & 75.9 & 48.1 & 75.7 & 75.8 & 73.1 \\
\rowstyle{\color{darkergray}}RG     & 73.2 & 72.0 & 73.2 & 73.7 & 45.0 & 70.8 & 71.9 & 74.0 \\
\rowstyle{\color{darkergray}}MC     & 78.3 & 75.7 & 78.2 & 78.2 & 46.5 & 77.5 & 76.0 & 80.2 \\}
T-SYN                               & \mb{56.4} & \mi{56.3} & \mi{56.2} & 51.2 & 37.6 & 50.5 & 54.4 & 46.0 \\
T-SEM                               & 34.3 & 34.3 & 34.3 & \mb{36.2} & 4.1  & 35.3 & 34.5 & 30.6 \\\remove{
\rowstyle{\color{darkergray}}TOEFL  & 82.5 & 82.5 & 82.5 & 71.2 & 45.0 & 85.0 & 82.5 & 65.0   }
  \end{tabular}
  \parbox{\textwidth}{\caption{Performance versus views removed from
      the multiview GCCA procedure. !Framenet means that the view
      containing counts derived from Frame semantic dataset was
      removed. Other columns are named similarly. The other
      hyperparameters were $n_j=\textrm{Count}^{\frac{1}{4}}, \;
      m=300, \; t=100K, \; v=25, \; k=300$. }}
\end{table*}
  
\noindent\textbf{Contribution of different sources of data}:
 Table~\ref{tab:j} shows an ablative analysis of performance where we
 remove individual views or some combination of them and measure the
 performance.  It is clear by comparing the last column to the second
 column that adding in more views 
 improves performance. Also we can see that the Dependency based views and the Bitext
 based views give a larger boost than the morphology and FrameNet
 based views, probably because the latter are so sparse.

\noindent\textbf{Comparison to other word representation creation methods:}
There are a large number of methods of creating representations both
multilingual and monolingual. We directly compare our method to the
monolingual systems Glove and Word2Vec as their performance is widely
considered to be the state of the art.
%% We acknowledge that there 
%% exist other methods such as the 3CosMul method proposed in
%% \cite{levy2014dependency} or the Multimodal neural embedding methods
%% presented recently in \cite{felix2014learning,weston2014hash} 
We trained the two systems on the English portion of the
\textit{Polyglot} Wikipedia dataset.\footnote{More specifically
we explicitly provided the vocabulary file to Glove and Word2Vec and set the
truncation threshold for word2Vec to 10. Also Glove was trained for 25
iterations and both methods used window of 15 previous words.}

To make a fair comparison we also provide 
results where we used only the views derived from the \textit{Polyglot}
wikipedia corpus. See column ``MVLSA (Monolingual)'' in Table~\ref{tab:c}. 
We also combined the embeddings created by different methods again
using GCCA and results are presented in columns titled ``Combined''. The Combined
models outperform all the three constituent methods which shows that
the three methods were learning different patterns from the
data and that pooling these experts by using GCCA was useful. Using GCCA itself for system combination provides closure
for the MVLSA algorithm since more and more algorithms can now be combined by using the same method.

%% for c in glove glove_0  word2vec; do \
%%            echo $$c ; $(MAKE) -s ts_extrinsic_"$$c"_mytrain_mycode; done
%%         echo "monolingMVLSA"; make -s ts_fullgcca_extrinsic_test_v5_embedding_mc_CountPow025-truncco\
%% l12500_500~E@mi@bi@ag@fn@mo,300_1e-5_170000.300.1.1 ; \
%%         echo "MVLSA"; make -s ts_fullgcca_extrinsic_test_v5_embedding_mc_CountPow025-trunccol12500_5\
%% 00~E@mi,300_1e-5_16.300.1.1 ;\
%%         echo "3combo"; make -s ts_combined_embedding_0; \
%%         echo "2combo"; make -s ts_combined_embedding_1 \

\begin{table*}[ht]
    \begin{adjustwidth}{-2cm}{}
      \rowcolors{1}{}{lightgray}
  \setlength\tabcolsep{2pt}
  \begin{tabular}{=l| +c +c +c | +c +c +c +c H H | +c +c H +c +c}
    Test Set &
    \specialcell{LC\\(0.05,0.7)} &
    Glove  &
    Word2Vec &
    \specialcell{MVLSA\\Monolingual} &
    \specialcell{MVLSA\\All Views}&
    \specialcell{$\rho$ \\Glove}&
    \specialcell{$\rho$ \\Word2Vec}&
    Increment  &
    Gap &
    \specialcell{Glove, Word2Vec\\Combined} &
    \specialcell{All\\Combined} &
    Increment &
    \specialcell{$\rho$ \\Glove}&
    \specialcell{$\rho$ \\Word2Vec}\\\hline
    
MEN                                & 2.3  & 70.4       & \myu{73.9} & 71.4 & 71.2 & 72 & 90 & -0.2      & -2.7  & 76.0 & \myu{75.8} & -0.2      & 84 & 92 \\
RW                                 & 2.8  & 28.1       & 32.9       & 29.0 & 41.7 & 70 & 71 & \mb{12.7} & 8.8   & 37.2 & \myu{40.5} & \mb{3.3}  & 77 & 73 \\
SCWS                               & 2.8  & 54.1       & \myu{65.6} & 61.8 & 67.3 & 72 & 86 & \mb{5.5}  & 1.7   & 60.7 & \myu{66.4} & \mb{5.7}  & 78 & 89 \\
SIMLEX                             & 4    & 33.7       & 36.7       & 34.5 & 42.4 & 61 & 77 & \mb{7.9}  & 5.7   & 41.1 & \myu{43.9} & 2.8       & 77 & 84 \\
WS                                 & 6.7  & 58.6       & \myu{70.8} & 68.0 & 70.8 & 69 & 87 & 2.8       & 0     & 67.4 & \myu{70.1} & 2.7       & 78 & 90 \\
MTURK                              & 7.5  & \myu{61.7} & \myu{65.1} & 59.1 & 59.7 & 70 & 77 & 0.6       & -5.4  & 59.8 & \myu{62.9} & 3.1       & 79 & 83 \\
WS-REL                             & 8    & 53.4       & \myu{63.6} & 60.1 & 65.1 & 58 & 80 & 5         & 1.5   & 59.6 & \myu{63.5} & 3.9       & 69 & 84 \\
WS-SEM                             & 8.9  & \myu{69.0} & \myu{78.4} & 76.8 & 78.8 & 72 & 89 & 2         & 0.4   & 76.1 & \myu{79.2} & 3.1       & 80 & 92 \\
\rowstyle{\color{darkergray}}RG    & 16   & \myu{73.8} & \myu{78.2} & 71.2 & 74.4 & 76 & 89 & 3.2       & -3.8  & 80.4 & \myu{80.8} & 0.4       & 89 & 89 \\
\rowstyle{\color{darkergray}}MC    & 23.9 & \myu{70.5} & \myu{78.5} & 76.6 & 75.9 & 77 & 93 & -0.7      & -2.6  & 82.7 & \myu{77.7} & -5        & 88 & 91 \\
T-SYN                              & 0.68 & 61.8       & 59.8       & 42.7 & 60.0 &    &    & \mb{17.3} & -1.8  & 51.0 & \myu{64.3} & \mb{13.3} &    &    \\
T-SEM                              & 0.74 & \myu{80.9} & 73.7       & 36.2 & 38.6 &    &    & \mb{2.4}  & -42.3 & 73.5 & 77.2       & \mb{3.7}  &    &    \\
\rowstyle{\color{darkergray}}TOEFL & 6.30 & \myu{83.8} & 81.2       & 78.8 & 87.5 &    &    & \mb{8.7}  & 3.7   & 86.2 & \myu{88.8} & 2.6       &    & 
  \end{tabular}
  \caption{Comparison of Word2Vec, Glove and Multiview LSA. Change in
    accuracy from the use of multiple views that is higher than LC is
    in bold and the top scoring entries from the Glove, Word2Vec and Combined(All)
  columns are underlined when the difference in their scores from the
  top one is less than LC.}
  \label{tab:c}
  \end{adjustwidth}
\end{table*}

\section{Previous Work}
\label{sec:previouswork}
Vector space representations of words have been created using diverse
 frameworks ranging from Spectral methods
 \cite{dhillon2011multi,dhillon2012two}
 %% \footnote{\url{cis.upenn.edu/~ungar/eigenwords}}
 to Neural Networks
 \cite{mikolov2013efficient,mikolov2013distributed,collobert2013word}
 %%\footnote{\url{code.google.com/p/word2vec},\url{metaoptimize.com/projects/wordreprs}}
 and trained using either one
 \cite{pennington2014glove}
 %% \footnote{\url{nlp.stanford.edu/projects/glove}}
 or two sources of cooccurrence statistics
 \cite{zou2013bilingual,faruqui2014improving,bansal2014tailoring,levy2014dependency}
 %% \footnote{\url{ttic.uchicago.edu/~mbansal/data/syntacticEmbeddings.zip,cs.cmu.edu/~mfaruqui/soft.html}}
 or using multi-modal data
 \cite{felix2014learning,bruni2012distributional}.
 
\cite{faruqui2014improving} demonstrated that bilingual
representations extracted using CCA outperformed their monolingual 
counterparts.
%% and clearly their method could be extended to
%% multiple languages by merging the representations a pair at a time,
%% though they did not do so in the paper.
More recently \cite{hill2014not} demonstrated this same
phenomenon through their experiments over neural representations learnt from MT
systems. Also \cite{dhillon2011multi,dhillon2012two} used
CCA as the primary method to learn vector representations. Our work
takes all these threads to work to a logical extreme and uses GCCA to learn distributed
representations using data from multiple languages.
%% Outside of the NLP
%% community \cite{sun2013generalized,tripathi2011data} are two
%% publications 
%% that we are aware of that have used GCCA for ``data fusion''.

While various researchers have tried to improve the
performance of their paraphrase systems or vector space models by using
multiple sources of information such as bilingual
corpora~\cite{bannard2005paraphrasing,Huang2012Improving,zou2013bilingual}, 
structured datasets~\cite{yu2014improving,faruqui2014retrofitting} or even
tagged images~\cite{bruni2012distributional}; 
%% The intuitive reason that using multiple sources of data improves performance is 
%% that the views complement each other. For example it was mentioned in
%% \cite{ganitkevitch2013ppdb} that monolingual data can't distinguish
%% between antonyms but bilingual data can. And bilingual data confounds
%% words that occur in the same sentence but monolingual data can
%% distinguish them based on their context.
However, the previous
work did not adopt the general, simplifying view that 
all of these sources of data are just cooccurrence 
statistics coming from different sources with underlying latent
factors.\footnote{Though \cite{faruqui2014retrofitting} use
  the sophisticated technique of belief propagation the graph that
  they use it on is a single undirected weighted information. That
  information can be perfectly captured in an adjacency matrix which is another type of
cooccurrence matrix. Also they do not fuse arbitrary views such as
other vector representations which we are performing.}

%% In the sense of being an application of multiview
%% learning methods to NLP our work is an addition to the long chain which
%% started from the work of \cite{yarowsky1995unsupervised} and continued
%% with Co-Training~\cite{blum1998combining}, CoBoosting~\cite{collins1999unsupervised} and ``2 view
%% perceptrons''~\cite{brefeld2006efficient}.  CCA is also an algorithm
%% for multi view learning \cite{kakade2007multi,ganchevuai08}, and it
%% has a probabilistic interpretation \cite{bach2005probabilistic} as well.

\section{Future Work}
\label{sec:futurework}
In a rough order of importance we believe we could improve our method in the following ways:
\begin{itemize}[leftmargin=*]
  \itemsep-0.1em
  \renewcommand\labelitemi{--}
\item By implementing the probabilistic version of GCCA which would
  allow us to create generative models which could be trained in an
  online fashion. We conjectured that ``MAXVAR'' formulation of GCCA is closely related to a probabilistic interpretation of GCCA. We are not aware of any work
that has made this connection before and we are working on proving this where the proof technique and style would closely mirror that in \cite{bach2005probabilistic}.
\item By using count dependent non-linear weighting as exemplified
  through Expression~\ref{eq:gcca3}
\item By implementing procedures for constant memory QR decomposition so that we can scale our method to larger vocabularies. 
\item By adding more views such as views that derived from successive
  words instead of  using only the previous words. Since we are not using
  any PMI-like or frequency like features there is scope for
  improvement on that front.  We note that the performance of our
  method on the ``T-SEM'' dataset was very low which we believe could
  have been caused due to lack of PMI type features have been reported
  to work well in \cite{levy2014neural}.  
%% \item by using kernel methods that promise to liberate us from tuning over non-linearities, though tuning over kernels is a problem as well.
\item By using more sophisticated method for handling missing values
  as mentioned earlier.
\item By performing discriminative optimization of multiplicative
  factors over the views. For example we saw that bitext views hurt
  performance on the ``T-SEM'' task whereas they improved performance
  on all the other tasks in general. A simple technique could be to
  simply assign a multiplicative factor to each view and 
  then to tune the values of that factor by using discriminative
  techniques. Of course such a method would not remain unsupervised any
  more. Such a discriminative optimization could help us understand
  better the reasons why a particular corpus improves performance on
  certain datasets but decreases performance on another dataset. %% Since
  %% we could calculate gradient of the gram matrix between words in a
  %% dataset versus the multiplicative factor.
\end{itemize}

\section{Conclusion}
Through the results in Table~\ref{tab:c} we demonstrated that fusing
multiple data sources through the procedure of GCCA is an effective
method to improve the performance of word representations. We also
demonstrated that GCCA could be used even over a monolingual dataset
thereby giving a more principled alternative of LSA which removed the
need for manually fusing the word-word coccurrence matrices through
reciprocal weighted averaging \cite{pennington2014glove}. We also
provided least counts of significant changes for the commonly used
datasets used to evalue word embeddings. 

%% The inter algorithm pearson correlation before and after performing
%% GCCA showed  we see that the
%% systems (word2vec and the Multiview LSA) are making the same types of
%% predictions so the systems are producing a very similar representation
%% even though the absolute performance on the extrinsic metrics differs
%% by a lot sometimes.  
\section*{Acknowledgments}
This material is based on research sponsored by Defense Advanced Research
Projects Agency (DARPA) under the Deep Exploration and
Filtering of Text (DEFT) Program (Agreement number
FA8750-13-2-0017). We also thank Juri Ganitkevitch for 
providing the word aligned bitext corpus used in this paper.

\bibliographystyle{naaclhlt2015}
\bibliography{references}
\end{document}

