\documentclass[11pt]{article}
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{color, colortbl}
\usepackage{amssymb}
\setlength{\parskip}{\baselineskip}
\renewcommand{\det}[1]{|#1|}
\newcommand{\mf}{-\frac{1}{2}}
\newcommand{\Mu}{M}
\newcommand{\hp}{\circ}
\newcommand{\fc}{\frac{1}{\sqrt{(2\pi)^{d+m}(\sigma^2)^m }}}
\newcommand{\todo}[1]{\textbf{\textcolor{red}{TODO: #1}}}
\newcommand{\note}[1]{\textbf{\textcolor{red}{NOTE: #1}}}
\newcommand{\ps}{\begin{bmatrix} \psi_1 & 0 \\ 0 & \psi_2 \end{bmatrix}}
\begin{document}
\raggedright
\section{Probabilistic PCA}

When I observe data then the best k orthogonal basis of a projection
space are the eigenvectors of the covariance matrix.

\begin{align}
&\arg\min ||X - UU^\top X ||^@_F \\
=&tr(M^\top M)\\
=&tr(X^TX + X^TUU^TUU^TX - zX^TUU^TX)\\
=& \max tr(X^TUU^TX) \\
=& tr(U^TXX^TU)
\end{align}
Now I can also try and solve the following problem.

I observe data x and I assume that it was generated in the following way:
\begin{align}
z \sim & \mathcal{N}(0, I_d)\\
x \sim & \mathcal{N}(Wz+\mu, \sigma^2 I_m)  
\end{align}
And I need to find $W, \mu, \sigma^2$ so that $p(x)$ is maximized. $W
\in \mathcal{R}^{m\times d}$.

\begin{align}
  p(x) &= \prod_{i=1}^n \int p(x_i | z_i) p(z_i)\\
  l(x) &= \sum_{i=1}^n \log(\int p(x_i|z_i) p(z_i)) \label{obj}\\
\end{align}
For simplicity lets drop subscripts and derive $p(x) = \int p(x|z) p(z)$.
\begin{align}
  p(z) &= \frac{1}{\sqrt{(2\pi)^d\det{I_d}}} \exp(-\frac{1}{2} z^T  I_d^{-1} z)\\
  p(x|z) &= \frac{1}{\sqrt{(2\pi)^m(\sigma^2)^m \det{I_m}}}
  \exp(-\frac{1}{2}(x-Wz-u)^T(\sigma^2I_m)^{-1}(x-Wz-\mu))
\end{align}

What is $\int p(x|z) p(z)$. I know it is a guassian. I also can use
mathematica. I can also use the fact that its solution is given in
kevin murphy's book. But lets derive it.
\begin{align}
  \int p(x|z)p(z) &= \fc \int
  \exp{\mf (z^Tz + (x-\mu - Wz)^T(\sigma^{-2})(x-\mu-Wz))}
\end{align}
Let $(x-\mu)/\sigma = y,\; W/\sigma=V$
\begin{align}
  \int p(x|z)p(z) &= \fc \int \exp(\mf (z^Tz + (y - Vz)^T(y-Vz)))\\
  &= \fc\exp(\mf y^Ty) \int \exp(\mf (z^T (I+V^TV) z  - 2z^TV^Ty))
\end{align}
Let $U = (I+V^TV)^{-1}$
\begin{align}
  &= \fc \exp(\mf y^Ty) \int \exp(\mf ( \left(z^TU^{-1}z -2z^TU^{-1}a + a^TU^{-1}a\right) - a^TU^{-1}a  ))
\end{align}
This means that $V^Ty = U^{-1}a \implies a = UV^Ty$
\begin{align}
  &= \fc \exp(\mf (y^Ty - a^TU^{-1}a) ) \int \exp(\mf ((z-a)^T U^{-1}(z-a)))\\
  &= \fc \sqrt{(2\pi)^d \det{U}} \exp(\mf (y^Ty - a^TU^{-1}a) )\\
  &= \fc \sqrt{(2\pi)^d \det{U}} \exp(\mf (y^Ty - y^TVU^TU^{-1}UV^Ty))\\
  &= \fc \sqrt{(2\pi)^d \det{U}} \exp(\mf (y^Ty - y^TVU^TV^Ty) )\\
  &= \fc \sqrt{(2\pi)^d \det{U}} \exp(\mf (y^T(I-VU^TV^T)y)
\end{align}

Just substitute back $y$ and $V$ and $U$ to get the answer
\begin{align}
  \int p(x|z)p(z) &= \frac{\sqrt{(2\pi)^d \det{U}}}{\sqrt{(2\pi)^{d+m}(\sigma^2)^m }} \exp(\mf ((x-\mu)^T\frac{(I-V(I+V^TV)^{-1}V^T)}{\sigma^2}(x-\mu))
\end{align}

Interestingly, $(I-V(I+V^TV)^{-1}V^T) = (I+VV^T)^{-1}$
This means that
\begin{align}
  p(z) = \int p(x|z)p(z) &= \frac{1}{\sqrt{(2\pi)^{m}(\sigma^2)^m \det{U}^{-1}}}  \exp(\mf ((x-\mu)^T (\sigma^2 (I+VV^T))^{-1}  (x-\mu)))
\end{align}
Also note that by Sylvester's theorem $\det{U} = \det{I+V^TV}^{-1} = \det{I+VV^T}^{-1} $.

So finally we have proven that $p(x) = \mathcal{N}(\mu,
\sigma^2I+WW^T)$

Now substitute this results back into \ref{obj} to get

\begin{align}
  l(x) &= \sum_{i=1}^n \log(\frac{1}{\sqrt{(2\pi)^{m}(\sigma^2)^m \det{I+VV^T}}}  \exp(\mf ((x_i-\mu)^T (\sigma^2 (I+VV^T))^{-1}  (x_i-\mu))))\\
  &= \mf \sum_{i=1}^n ((x_i-\mu)^T (\sigma^2I+WW^T))^{-1}  (x_i-\mu)) + \log((2\pi)^{m} \det{\sigma^2I+WW^T})
\end{align}
Now get rid of constant terms like $m\log(2\pi)$ and the constant
multipliers which don't affect the optimization (However now I have to
minimize the quantity since I removed $\mf$)
\begin{align}
  &= \sum_{i=1}^n ((x_i-\mu)^T (\sigma^2I+WW^T)^{-1}  (x_i-\mu)) + \log(\det{\sigma^2I+WW^T})
\end{align}
Now minimize this quantity wrt $\mu, W, \sigma$ to get the MLE
parameters

Wrt $\mu$\\
Let $Y=(\sigma^2I+WW^T))^{-1}$, then wrt $\mu$ the derivative is
\begin{align}
  =& \sum_{i=1}^n \frac{\partial}{\partial \mu}  ((x_i-\mu)^T Y  (x_i-\mu)) \\
  =& \sum_{i=1}^n \frac{\partial}{\partial \mu}  (\mu^T Y\mu -
  2x_i^TY\mu + x_i^TYx_i)) \\
  =& \sum_{i=1}^n  (Y+Y^T)\mu -  (2x_i^TY)^T )) \\
\end{align}
However $Y$ is symmetric, therefore $Y+Y^T = 2Y^T$, which means
$$\mu = \frac{1}{n}{\sum_{i=1}^n x_i}$$


Let $y_i = x_i - \mu$ then wrt $\sigma$ and $W$ the objective to
minimize is: where $\sum y_i = 0$
\begin{align}
  =& \sum_{i=1}^n \left(  (y_i^T
  (\sigma^2I+WW^T)^{-1}  y_i) + \log(\det{\sigma^2I+WW^T}) \right)\\
\end{align}
Let's level up and concatenate all the $y_i$ into a matric $Y$, then
the objective becomes
\begin{align}
  tr(Y^T (\sigma^2I+WW^T)^{-1}Y) + n\log(\det{\sigma^2I+WW^T})
\end{align}
Interestingly $\sigma^2I+WW^T$ are inseparable, replace them by $V = (\sigma^2I+WW^T)^{-1}$ to
get the new objective
\begin{align}
  \arg\min_V tr(Y^TVY) - n\log(\det{V})
\end{align}

\begin{align}
  \frac{\partial}{\partial V} tr(Y^TVY) - n\log(\det{V}) &= YY^T - n
  V^{-1} = 0\\
  V^{-1} = (\sigma^2I+WW^T) &= \frac{1}{n}YY^T
\end{align}

Now to solve this equation we would have to let $\sigma$ equal the
last $d-m$ eigenvalues so they would equal each other and then we can
 do an eigen decomposition
of $C = YY^T/n - \sigma^2I = U\sqrt{S}MM^T\sqrt{S^T}U^T \in \mathcal{R}^{d \times
  d}$ then we derive the PCA solution assuming that we had perfect
knowledge of the ppopulation means.

But if instead of taking derivative wrt $V$ we take derivative wrt $W$
and actually recognize the constraint that $W$ would not be able to
perfectly match $C=YY^T/n$  then we would get: \todo{COMPLETE THIS}
\begin{align}
  (YY^T/n(\sigma^2I+WW^T))W &= W
\end{align}

\section{Probabilistic CCA}
I will use the following objective as my starting point for CCA.
Let $A=\Sigma_{xx}, B=\Sigma_{YY}, C=\Sigma_{XY}$
\begin{align}
  \arg\max_{U,V} \;& tr(U^T C V)\\
  \text{subject to } & U^TAU=I,\; V^TBV=I
\end{align}
We can solve this in two ways.
\subsection{Approach 1}
The usual approach is to first find a single direction and then show
that the remaining directions follow as the remaining eigen-vectors of
a matrix all of which would follow the required constraints. Let's
recap that approach first.

Let $u,\; v$ be two directions, then the lagrangian of the objective
becomes
\begin{align}
  \mathcal{L} &=u^TCv + \lambda (u^tAu-1 ) + \mu (v^TBv-1)\\
  \frac{\partial \mathcal{L}}{\partial u} &= v^TC^T + u^T(2\lambda A)  =0 = Cv + 2\lambda A^Tu\\
  \frac{\partial \mathcal{L}}{\partial v} &= u^TC + v^T(2\mu B)  =0=C^Tu+2\mu B^T v\\
  u &= -\frac{1}{2\lambda}A^{-T}Cv\\
  -C^T (-\frac{1}{2\lambda}A^{-T}Cv) &= 2\mu B^Tv\\
  B^{-T}C^TA^{-T}Cv &=4\lambda \mu v
\end{align}
Now use $B=B^T,\; A=A^T,\; 4\lambda \mu = \kappa$, also use the fact that B can be decomposed
into its cholesky factors $B=DD^T \implies B^{-1}=D^{-T}D^{-1}$ to simplify the objective:
\begin{align}
  D^{-T}D^{-1}C^TA^{-1}CD^{-T}D^Tv &=\kappa v  \\
  D^{-1}C^TA^{-1}CD^{-T}D^Tv &=\kappa D^Tv  
\end{align}
Let $v'=D^Tv$ to get
\begin{align}
  D^{-1}C^TA^{-1}CD^{-T}v' &=\kappa v'  \\
  \text{Let } M &= D^{-1}C^TA^{-1}CD^{-T}
\end{align}
Now we claim that the successive orthogonal eigen vectors $v'$ of the
symmateric matrix $M$ would give us the
solutions as $v=D^{-T}v', \; u = -\frac{1}{2\lambda}A^{-T}CD^{-T}v'$.

It is easy to see that $v^TBv = v'^T D^{-1}BD^{-T}v' = 1$ and all
subsequent vectors would also obey this.

We can check that $u^TAu =
\frac{1}{4\lambda^2}v'^TD^{-1}C^TA^{-1}AA^{-T}CD^{-T}v' =
\frac{1}{4\lambda^2}v'^TMv'=\frac{\mu}{\lambda}$
We are free to choose $\lambda, \mu$ so to satisfy the constraints we
should choose $\mu=\lambda$.
\subsection{Approach 2}
This approach adds one lagrange multiplier per constraint
and uses hard algebra to grind through. Specifically the lagrangian becomes
\begin{align}
  L &= tr(U^TCV) + \sum \lambda_{ij}(u_i^TAu_j - \delta_{ij}) + \sum
  \mu_{ij}(v_i^TBv_j - \delta_{ij})\\
  &= tr(U^TCV) + 1^T U^T(A\hp \Lambda)U 1 - tr(\Lambda) + 1^T V^T(B\hp \Mu)V 1 - tr(\Mu) \\
  &= tr(U^TCV + 1^T U^T(A\hp \Lambda)U 1 - \Lambda + 1^T V^T(B\hp  \Mu)V 1 - \Mu)\\
  \frac{\partial L}{\partial U}&= V^TC^T + 11^TU^T(A\hp (\Lambda + \Lambda)^T) = 0
\end{align}
\todo{Confirm that this method would work}
\subsection{Probabilistic CCA}
Basically we have to find the MLE estimates of the parameters of a two
observed, one hidden variable model defined with the following
generative story. 

I observe data coming in two pairs and I assume that it was generated
in the following way
\begin{align}
  z &\sim \mathcal{N}(0, I_d)\\
  x &\sim \mathcal{N}(W_1z + u_1, \psi_1)\\
  y &\sim \mathcal{N}(W_2z + u_2, \psi_2)
\end{align}
Now I want to estimate the parameters $W_1, W_2, \psi_1, \psi_2,
u_1, u_2$ on the basis of the data. So basically I need to
maximize the likelihood of the parameters.
\begin{align}
  \mathcal{L} &= \prod_i \int_z p(x_i|z) p(y_i|z) p(z)\\
  \log(\mathcal{L}) &= \sum_i \log(\int_z p(x_i|z) p(y_i|z) p(z))
\end{align}
Lets focus on $I = \int_z p(x|z) p(y|z) p(z)$. Let $\dim(x)=m\; \dim(y)=q$
\begin{align}
  p(z)&= \frac{1}{\sqrt{(2\pi)^d}} \exp(\mf z^Tz)\\
  p(x|z)&= \frac{1}{\sqrt{(2\pi)^m\det{\psi_1}}}  \exp( \mf (x-W_1z-u_1)^T\psi_1^{-1}(x-W_1z-u_1))\\
  p(y|z)&= \frac{1}{\sqrt{(2\pi)^q\det{\psi_2}}}  \exp( \mf (y-W_2z-u_2)^T\psi_2^{-1}(y-W_2z-u_2))\\
  I &= \int_z p(x|z) p(y|z) p(z)\\
  &= \frac{1}{\sqrt{(2\pi)^{d+m+q} \det{\psi_1}\det{\psi_2} }}\int_z \exp(\mf
  \big( z^Tz + (x-W_1z-u_1)^T\psi_1^{-1}(x-W_1z-u_1) + \notag\\
  & \qquad (y-W_2z-u_2)^T\psi_2^{-1}(y-W_2z-u_2) \big) ) 
\end{align}
Now the basic idea is the same as probabilistic PCA, the final
probability distribution is joint guassian over $x, y$, and then we
want to maximize the likelihood of parameters.

Let $a = x-u_1, \; b = y-u_2$ and focus only on the quadratic term $T$.
\begin{align}
  T &= z^Tz + (a-W_1z)^T\psi_1^{-1}(a-W_1z) +
  (b-W_2z)^T\psi_2^{-1}(b-W_2z)\\
  (a-W_1z)^T\psi_1^{-1}(a-W_1z) &= (a^T\psi_1^{-1}a)+  (z^TW_1^T\psi_1^{-1}W_1z) - 2z^TW_1^T\psi_1^{-1}a\\
  (b-W_2z)^T\psi_2^{-1}(b-W_2z) &= (b^T\psi_2^{-1}b)+  (z^TW_2^T\psi_2^{-1}W_2z) - 2z^TW_2^T\psi_2^{-1}b\\
  T &= z^T(I+W_1^T\psi_1^{-1}W_1+W_2^T\psi_2^{-1}W_2)z -2z^T(W_1^T\psi_1^{-1}a + W_2^T\psi_2^{-1}b) + \notag\\
  & \qquad ((a^T\psi_1^{-1}a) + (b^T\psi_2^{-1}b))
\end{align}
Once again we have to complete the squares which means that we'd have to match $T$ above to an expression like the following
\begin{align}
  (z-\mu)^T V^{-1}(z-\mu)=  (z^T V^{-1}z) -2z^TV^{-1}\mu + (\mu^T V^{-1}\mu)
\end{align}
Which means that $V = (I+W_1^T\psi_1^{-1}W_1+W_2^T\psi_2^{-1}W_2)^{-1}$ and $V^{-1}\mu = (W_1^T\psi_1^{-1}a + W_2^T\psi_2^{-1}b) \implies \mu = V(W_1^T\psi_1^{-1}a + W_2^T\psi_2^{-1}b)$.

Let $c = ((a^T\psi_1^{-1}a) + (b^T\psi_2^{-1}b)) - (\mu^T V^{-1}\mu)$, then basically the integral $I$ becomes the following:
\begin{align}
  I &= \frac{1}{\sqrt{(2\pi)^{d+m+q} \det{\psi_1}\det{\psi_2} }}
  \exp(\mf c) \int \exp(\mf (z-\mu)^T V^{-1}(z-\mu))\\
  &= \frac{\sqrt{\det{V}}}{\sqrt{(2\pi)^{m+q} \det{\psi_1}\det{\psi_2} }} \exp(\mf c)\\
  &= ((2\pi)^{m+q}
  \det{\psi_1}\det{\psi_2}\det{V}^{-1})^{-1/2}\exp(\mf c) \label{ccasimple}
\end{align}

Of course we only want to maximize the real likelihood.
\begin{align}
  \log(\mathcal{L})&= \sum \log(I_i) \\
  &= \mf (\sum_i c_i + n \log((2\pi)^{m+q} \det{\psi_1}\det{\psi_2}\det{V}^{-1}))
\end{align}
After removing constants we get the following objective
\begin{align}
  & \arg\min (\sum_i c_i + n \log(\det{\psi_1}\det{\psi_2}\det{I+W_1^T\psi_1^{-1}W_1+W_2^T\psi_2^{-1}W_2}))\\
  c_i &= ((a_i^T\psi_1^{-1}a_i) + (b_i^T\psi_2^{-1}b_i)) - (\mu_i^T V^{-1}\mu_i)\\
  \mu_i &= V(W_1^T\psi_1^{-1}a_i + W_2^T\psi_2^{-1}b_i)\\
  V &= (I+W_1^T\psi_1^{-1}W_1+W_2^T\psi_2^{-1}W_2)^{-1}\\
  \implies c_i &= a_i^T\psi_1^{-1}a_i + b_i^T\psi_2^{-1}b_i - (W_1^T\psi_1^{-1}a_i + W_2^T\psi_2^{-1}b_i)^T V^T(W_1^T\psi_1^{-1}a_i + W_2^T\psi_2^{-1}b_i)
\end{align}

Now strictly speaking one could muddle his/her way through by computer
assisted algebra but it would be better to simplify the forms.

The trick is to go back to Expression~\ref{ccasimple} and to then
view it as a higher dimensional Guassian. Specifically
\begin{align}
  I &= \frac{1}{\sqrt{(2\pi)^{m+q}\det{\psi_1}\det{\psi_2}\det{V}^{-1}}}\exp(\mf c)\\
  &=
  \frac{1}{\sqrt{(2\pi)^{m+q}\det{\psi_1}\det{\psi_2}\det{V}^{-1}}}\exp(\mf
  ((a^T\psi_1^{-1}a) + (b^T\psi_2^{-1}b)) - (\mu^T V^{-1}\mu))\\
  &= \frac{1}{\sqrt{(2\pi)^{m+q}\det{\psi_1}\det{\psi_2}\det{V}^{-1}}}\exp(\mf ((a^T\psi_1^{-1}a) + (b^T\psi_2^{-1}b)) - (W_1^T\psi_1^{-1}a + W_2^T\psi_2^{-1}b)^T V^T(W_1^T\psi_1^{-1}a + W_2^T\psi_2^{-1}b))
\end{align}
Now consider just the quadratic term Q
\begin{align}
  Q &= a^T\psi_1^{-1}a + b^T\psi_2^{-1}b - (W_1^T\psi_1^{-1}a +
  W_2^T\psi_2^{-1}b)^T V^T(W_1^T\psi_1^{-1}a + W_2^T\psi_2^{-1}b)\\
  Q &= a^T(\psi_1^{-1} - \psi_1^{-T}W_1V^TW_1^T \psi_1^{-1}) a +
  b^T(\psi_2^{-1} - \psi_2^{-T}W_2V^TW_2^T \psi_2^{-1}) b  \notag\\
  & \qquad - b^T\psi_2^{-T}W_2V^TW_1^T\psi_1^{-1}a -
  a^T\psi_1^{-T}W_1V^TW_2^T\psi_2^{-1}b\\
  &= \begin{bmatrix} a^T & b^T \end{bmatrix}
    \begin{bmatrix}
       \psi_1^{-1} - \psi_1^{-T}W_1V^TW_1^T \psi_1^{-1} & -\psi_1^{-T}W_1V^TW_2^T\psi_2^{-1} \\
       -\psi_2^{-T}W_2V^TW_1^T\psi_1^{-1}  & \psi_2^{-1} -\psi_2^{-T}W_2V^TW_2^T\psi_2^{-1}  \end{bmatrix}
    \begin{bmatrix}a    \\ b \end{bmatrix}\\
    &= \begin{bmatrix} a^T & b^T \end{bmatrix}
    (\ps^{-1} - \begin{bmatrix}
        \psi_1^{-T}W_1V^TW_1^T \psi_1^{-1} & \psi_1^{-T}W_1V^TW_2^T\psi_2^{-1} \\
       \psi_2^{-T}W_2V^TW_1^T\psi_1^{-1}  & \psi_2^{-T}W_2V^TW_2^T\psi_2^{-1}  \end{bmatrix})
    \begin{bmatrix}a    \\ b \end{bmatrix}\\
    &= \begin{bmatrix} a^T & b^T \end{bmatrix}
    (\ps^{-1} - \ps^{-1} \begin{bmatrix}
        W_1V^TW_1^T  & W_1V^TW_2^T \\
       W_2V^TW_1^T  & W_2V^TW_2^T  \end{bmatrix}\ps^{-1})
    \begin{bmatrix}a    \\ b \end{bmatrix} \\
    &= \begin{bmatrix} a^T & b^T \end{bmatrix}
    (\ps^{-1} - \ps^{-1}
    \begin{bmatrix} W_1 \\ W_2 \end{bmatrix}
    V
    \begin{bmatrix} W_1^T & W_2^T \end{bmatrix}
    \ps^{-1})
    \begin{bmatrix}a    \\ b \end{bmatrix}
\end{align}
Now note that $\psi$ and $V$ are both symmetric. Also note that
\begin{align}
  V &= (I+W_1^T\psi_1^{-1}W_1+W_2^T\psi_2^{-1}W_2)^{-1}\\
  &= (I + \begin{bmatrix} W_1^T W_2^T \end{bmatrix} \begin{bmatrix}
    \psi_1 & 0 \\ 0 & \psi_2 \end{bmatrix}^{-1} \begin{bmatrix} W_1
    \\ W_2 \end{bmatrix})^{-1}
\end{align}
Also consider
\begin{align}
  \Sigma &= \begin{bmatrix} \psi_1 & 0 \\ 0 &
    \psi_2 \end{bmatrix} + \begin{bmatrix} W_1 \\ W_2 \end{bmatrix}  \begin{bmatrix} W_1^T & W_2^T \end{bmatrix}
\end{align}
By the woodbury identity we know its inverse is as follows
\begin{align}
  \Sigma^{-1} &= \ps^{-1} - \ps^{-1}\begin{bmatrix} W_1
    \\ W_2 \end{bmatrix}(I+ \begin{bmatrix} W_1^T &
    W_2^T \end{bmatrix} \ps^{-1} \begin{bmatrix} W_1
    \\ W_2 \end{bmatrix})^{-1} \begin{bmatrix} W_1^T &
    W_2^T \end{bmatrix} \ps^{-1}
\end{align}
The determinant of $\Sigma$ can also be shown to be equal to the form
in the square root by using the fact that $$\det{A+UV^T} = \det{I+V^TA^{-1}U}\det{A}$$

Once we have done this derivation then we can move forward by
calculating the loglikelihood for a general guassian.
Let $u = [u_1; u_2]$, and $v_i = [x_i; y_i]$
\begin{align}
  \mathcal{-L}  &\sim n\log(\det{\Sigma}) + \sum_i tr((v_i-u)^T\Sigma^{-1}(v_i-u))
\end{align}
Now we can cycle the things inside the trace and take out constant
things from the summation and replace $\sum v_i/n = \xi$ and $\sum
(v_iv_i^T - \xi \xi^T)/n = \Xi$ also replace $v_i - u = v_i - \xi +
\xi - u$

\begin{align}
  \mathcal{-L}  &\sim \log(\det{\Sigma}) + tr(\Sigma^{-1}\Xi)+(\xi -
  u)^T\Sigma^{-1}(\xi - u) \label{ccal}
\end{align}

Now again we have to minimize Expression~\ref{ccal} and $\xi, \Xi$ are
given to us through the data. It is easy to set $u$ but $\Sigma$ needs
to be chosen more carefully because it is constrained to be of the
form
\begin{align}
  \Sigma &= \begin{bmatrix} \psi_1 & 0 \\ 0 & \psi_2 \end{bmatrix} + \begin{bmatrix} W_1 \\ W_2 \end{bmatrix}  \begin{bmatrix} W_1^T &    W_2^T \end{bmatrix}\\
  \Sigma &= \begin{bmatrix} \psi_1 & 0 \\ 0 &
    \psi_2 \end{bmatrix} + W W^T
\end{align}

I repeat that the problem only occurs because $\Sigma$ needs to be of
a certain form. If it could have been a general matrix then I wouldn't
have any problems. I'd just take the derivative of
expression~\ref{ccal} and set it to 0. In any case the term
$(\xi - u)^T\Sigma^{-1}(\xi - u)$ is zero so we dont need to worry
about its derivative.

Now let's take derivative of $-\mathcal{L}$ wrt to $\psi_1,
\psi_2, W=[W_1; W_2]$. We must work with $\det{\Sigma}\ne 0$

Wrt $\psi_1$:
\begin{align}
  \partial \mathcal{L} &= tr(\Sigma^{-1} \partial \Sigma - \Xi\Sigma^{-1}\partial\Sigma\Sigma^{-1})\\
  &=  tr((\Sigma^{-1}  - \Sigma^{-1}\Xi\Sigma^{-1})\partial\Sigma)\\
  &\implies (\Sigma^{-1}  - \Sigma^{-1}\Xi\Sigma^{-1}) \begin{bmatrix}I & 0\\ 0 & 0\end{bmatrix} = 0
\end{align}

Wrt $\psi_2$:
\begin{align}
  \partial \mathcal{L} &= tr(\Sigma^{-1} \partial \Sigma - \Xi\Sigma^{-1}\partial\Sigma\Sigma^{-1}) = 0\\
  &=  tr((\Sigma^{-1}  - \Sigma^{-1}\Xi\Sigma^{-1})\partial\Sigma)\\
  &\implies (\Sigma^{-1}  - \Sigma^{-1}\Xi\Sigma^{-1}) \begin{bmatrix}0 & 0\\ 0 & I\end{bmatrix} = 0
\end{align}

Wrt $W$:
\begin{align}
 \partial \mathcal{L} &= (\Sigma^{-1}  - \Sigma^{-1}\Xi\Sigma^{-1})W = 0
\end{align}

Now we need to find $\psi, W$ that satisfy these three constraints and
later check which of those is the minima ? 
In order to find the solutions Bach first studies the solutions. He
says that assume you that such poins exist. Then those points 
first satisfy some conditions. 
\begin{enumerate}
\item $WW^T \preccurlyeq \Xi$
\item $\Psi = \Sigma\Xi^{-1}(\Xi - WW^T)$
\item $\psi_1 = \Xi_{11} - W_1 W_1^T$ and $\psi_1 = \Xi_{22} - W_1 W_1^T$
\end{enumerate}

But of course he didnt come up with these relations from thin air. It
is conceivable that he did try to study the solution but the
conditions themselves would have come as a results of search. Anyway
let's derive some of the lemmas to get comfortable with them

\begin{align}
           & (\Sigma^{-1}  - \Sigma^{-1}\Xi\Sigma^{-1})W = 0           \\
  \implies & W = \Xi\Sigma^{-1}W \\
  \implies & W = \Xi(\Psi + WW^T)^{-1}W \\
  \implies & W = \Xi(\Psi^{-1} - \Psi^{-1} W(I+W^T\Psi^{-1}W)^{-1}W^T\Psi^{-1})W \label{eq1}\\
  \implies & W = \Xi(\Psi^{-1}W - \Psi^{-1} W(I+W^T\Psi^{-1}W)^{-1}W^T\Psi^{-1}W) \label{eq2}\\
\end{align}
Now let $W^T\Psi^{-1}W = V$, then Expression~\ref{eq2}
\begin{align}
  W &= \Xi(\Psi^{-1}W - \Psi^{-1} W(I+V)^{-1}V) \\
  W &= \Xi\Psi^{-1}W(I - (I+V)^{-1}V)\\
  W &= \Xi\Psi^{-1}W(I + V)^{-1}\\
  W +WV &= \Xi \Psi^{-1}W\\
  W + WW^T\Psi^{-1}W &= \Xi \Psi^{-1}W\\
  W &= (\Xi - WW^T) \Psi^{-1}W\\
  \implies \Psi &= \Xi - WW^T \;\text{\todo{If we assume that $WW^T$ is invertible}}\\
\end{align}
Now do the cholesky decomposition $\Psi^{-1} = M^TM$ and apply to Expression!\ref{eq1}
\begin{align}
  W &= \Xi(M^TM - M^T M W(I+W^T M^T MW)^{-1}W^T M^T M)W \\
\end{align}
Let $V = MW$ to get
\begin{align}
  W &= \Xi M^TMW - \Xi M^T M W(I+W^T M^T MW)^{-1}W^T M^T MW \\
  &= \Xi M^TV - \Xi M^T V(I+V^TV)^{-1}V^T V \\
  V &= M\Xi M^TV(I - (I+V^TV)^{-1}V^TV)
\end{align}
Since $\Psi$ is invertible so $M$ is invertible.
\begin{align}
  M^{-1}V &= \Xi M^TV(I - (I+V^TV)^{-1}V^TV)
\end{align}

No strike this he derivation goes as follows
\begin{align}
  W &= \Xi(\Psi + WW^T)^{-1}W \\
  W &= \Xi(\Psi^{1/2}\Psi^{1/2} + \Psi^{1/2}\Psi^{-1/2}WW^T\Psi^{-1/2}\Psi^{1/2})^{-1}W \\
  W &= \Xi\Psi^{-1/2}(I + \Psi^{-1/2}WW^T\Psi^{-1/2})^{-1}\Psi^{-1/2}W \\
  \Psi^{-1/2}W &= \Psi^{-1/2}\Xi\Psi^{-1/2}(I + \Psi^{-1/2}WW^T\Psi^{-1/2})^{-1}\Psi^{-1/2}W \\
  \Psi^{-1/2}WW^T\Psi^{-1/2} &= \Psi^{-1/2}\Xi\Psi^{-1/2}(I + \Psi^{-1/2}WW^T\Psi^{-1/2})^{-1}\Psi^{-1/2}WW^T\Psi^{-1/2} \\
\end{align}
Now note that $\Psi^{-1/2}WW^T\Psi^{-1/2}$ is psd therefore it has a eigen-decomposition $USU^T$.
If we ASSUME that it is pd then we can say that 
\begin{align}
  I &= \Psi^{-1/2}\Xi\Psi^{-1/2}(I+USU^T)^{-1}\\
  (I+USU^T) &= \Psi^{-1/2}\Xi\Psi^{-1/2} \\
  \implies \Xi  &\succcurlyeq \Psi^{1/2}USU^T\Psi^{1/2} = WW^T
\end{align}

\todo{It is not clear why we can assume that $WW^T$ which is a psd matrix is spd, but go ahead.}
 
\todo{Actually complete the derivation.}

\section{Incremental Statistical language models}
\subsection{PPCA as a language model}
It is not possible since it only has one view of data. and I do not
want to model the next word as the hidden state rather I want to say
that the next word is a view that is hidden from us and it is
connected to the visible view through a latent state.  

\subsection{Using PCCA as a language model}
\todo{Anything that I can do with PGCCA I can do with PCCA. It makes
  sense to derive those things in the easier case and then to forge
  ahead, Think about complexity, performance, advantages etc. at this
  point.}

To do this I need to derive p(view 2| view 1) and ensure that it can
be computed efficiently. p(view 2 | view 1) would become the guassian
language model. 
So basically
\begin{align}
  p(y|x) &= \frac{p(y,x)}{p(x)} \\
  &= \frac{\int p(y,x,z)}{p(x)}\\
  &= \frac{\int p(y,x|z)p(z)}{p(x)}\\
  &= \frac{\int p(y|z)p(x|z)p(z)}{p(x)}\\
\end{align}
Both the numerator and the denominator I have calculated before so
plug those values in to get $p(y|x)$ in closed form.

Now the joint pdf of (view-1, view-2) that we calculated was joint normal with
the following multivariate normal distribution.

\pagebreak
Under probabilistic interpretation of CCA two views have the joint pdf as follows
\begin{align}
  \mu &= [\tilde{\mu}_1; \tilde{\mu}_2]\; \text{The sample mean.}\\
  \Sigma &= \begin{bmatrix} W_1 W_1^T + \Psi_1 & W_1 W_2^T \\ W_2 W_1^T & W_2 W_2^T + \Psi_2\end{bmatrix}\\
  \Psi_1 &= \tilde{\Sigma}_{11} - W_1 W_1^T\\
  \Psi_2 &= \tilde{\Sigma}_{22} - W_2 W_2^T\\
  W_1 &= \tilde{\Sigma}_{11}U_{1d}\\
  W_2 &= \tilde{\Sigma}_{22}U_{2d}\\
  U_{1d} &= \tilde{\Sigma}_{11}^{-1/2} \text{lsv}_d(\tilde{\Sigma}_{11}^{-1/2}\tilde{\Sigma}_{12}\tilde{\Sigma}_{22}^{-1/2})\text{lsv=left singular vector}\\
  U_{2d} &= \tilde{\Sigma}_{22}^{-1/2} \text{rsv}_d(\tilde{\Sigma}_{11}^{-1/2}\tilde{\Sigma}_{12}\tilde{\Sigma}_{22}^{-1/2})\\
  \text{Sample Covariance} &= \begin{bmatrix} \tilde{\Sigma}_{11} &
    \tilde{\Sigma}_{12} \\ \tilde{\Sigma}_{21} & \tilde{\Sigma}_{22} \end{bmatrix}
\end{align}
We know that $E[V_2 | V_1]$ is the following:
$$E[V_2 | V_1] = \tilde{\mu}_2 + (W_2W_1^T)(W_1W_1^T+\Psi_1)^{-1}(V_1 - \mu_2)$$
$$p(V_2 | V_1) = \mathcal{N}(E[V_2 | V_1], W_2W_2^T+\Psi_2 - W_2W_1^T(W_1W_1^T+\Psi_1)^{-1}W_1W_2^T)$$
So now if we want to get a probability then we can explicitly calculate it.

So assume that I observe the following two views \note{Both k and n are hyper-param}
\begin{align}
  \text{View-1: }\begin{bmatrix}1 0 1 0 0 0 1 \ldots\end{bmatrix}\\
  \text{View 1 contains words 1 to n-1}\\
  \text{View-2: }\begin{bmatrix}0 0 0 0 1 0 0 \ldots\end{bmatrix}\\
  \text{View 2 contains the nth word}
\end{align}
%% Then we can estimate $\alpha = E[\text{View 2} | \text{View 1}]$ and
%% $\beta = \max(\alpha - \text{View 1})$.
So at test time my observations are encoded in view 1, which are encoded into a
V-long count vector. Now I want to do prediction of view 2 which can then be used for  
\section{Probabilistic GCCA}
We want to derive probabilistic GCCA because then we can give a generative semantics to my embeddings and use them for language modelling.
\todo{TODO}
\end{document}
